{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq1WZzCeLqpK"
   },
   "source": [
    "Your name: Huy Tran\n",
    "\n",
    "Your student ID number: 33259590\n",
    "\n",
    "Shared link to this notebook: https://github.com/hvtran21/cs446_p3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhlgD8H1Ll5U"
   },
   "source": [
    "# Programming Assignment 3 (P3) for COMPSCI 446, Search Engines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9_kEMGent_J"
   },
   "source": [
    "This project is focused on indexing and query processing using a small collection of documents. Before starting, be sure to review Chapter 5 in the textbook, especially section 5.3, which covers inverted indexes, and section 5.6 (with an emphasis on 5.6.1), which discusses index construction. For the query processing part, sections 5.7.1 and 5.7.2, as well as sections 7.1.1, 7.2.2, and 7.3.1, will be particularly useful.\n",
    "\n",
    "Your task is to implement `run_queries(document_fpath, query_fpath, trecrun_file)` method. This method will accomplish 3 things:\n",
    "* [Document Parsing](#document-parsing) : Parse the collection of documents and create an inverted index.\n",
    "* [Query Processing](#query-processing) : Read and process queries from `query_fpath`.\n",
    "* [TREC Run Formatting](#output-formatting) : Output the results in TREC run format (as in P2) to `trecrun_file`.\n",
    "\n",
    "Only the implementation of `run_queries(document_fpath, query_fpath, trecrun_file)` will be graded. However, we will provide a skeleton of helper methods you can implement for this project. Call any helper functions from run_queries. Try to initialize any variables you need in run_queries (and do not use global variables). We will not test the provided helper functions or even verify that they exist. (Though please be certain that those code cells run, even if you do that by making an empty function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwNGCWtxoIXd"
   },
   "source": [
    "Here are a list of provided files that will be loaded into your Google Drive for use in this notebook:\n",
    "\n",
    "* _sciam.json.gz_, a compressed document collection comprising of a number of stories from issues of Scientific American from the 1800s. They are all from Project Gutenberg’s collection. The “books” were automatically broken into stories and each story is a “document” for purposes of this project. A URL was also automatically included that takes you to a nice version of the story if you’re interested in looking at it (or its images). These stories have been partially vetted for objectionable material (sensibilities in the 1800s were different than they are today); if you find something that we overlooked, please let us know.\n",
    "\n",
    "* _P3train.tsv_, a file containing a list of sample queries to run as input to your program for testing.\n",
    "\n",
    "* _P3train.trecrun_ is a file containing a sample, trecrun-formatted output for _P3train.tsv_.\n",
    "\n",
    "* _P3eval.tsv_ is the file containing more queries that P3 would be evaluated on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwtOQuDMHIOy"
   },
   "source": [
    "##0. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "hWAYQYm7AiFQ"
   },
   "outputs": [],
   "source": [
    "version = 4 # DO NOT MODIFY. If notebook does not match with autograder version, many tests are likely to fail!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-rdywfVn8Zd"
   },
   "source": [
    "###Link your Google Drive\n",
    "Execute the following to connect to Google Drive (you will be prompted repeatedly for access to your Google Drive; please give it permission) and download copies of the sample files listed above. You should not need to make any modifications to the code, though if you want to use a slightly different path in Google Drive, you can modify the appropriate drive_path value. (The autograder will not use your Google Drive and will fail if you insert code that explicitly uses the Google Drive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "id": "NOrBJqzHcaul"
   },
   "outputs": [],
   "source": [
    "# Import packages one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15499,
     "status": "ok",
     "timestamp": 1733688056372,
     "user": {
      "displayName": "Huy Tran",
      "userId": "16988764098922800274"
     },
     "user_tz": 300
    },
    "id": "0RQVcpdzFG08",
    "outputId": "5c5ceed0-bbf2-4669-a858-6efb292c5b51"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    in_colab = True\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "\n",
    "\n",
    "# You are more than welcome to code some helper functions.\n",
    "# But do note that we are only grading functions that are coded in the template files.\n",
    "\n",
    "\n",
    "# Connect to Google Drive and download copies of the sample files listed above.\n",
    "# Please allow the access to your Google Drive or the following dataset loader will fail.\n",
    "# (The autograder will not use your Google Drive.)\n",
    "if in_colab:\n",
    "  drive.mount(\"/content/drive/\") ## DO NOT MODIFY THIS LINE\n",
    "  data_path = \"/content/drive/MyDrive/CS 446 Search Engines/p3\" ## CHANGE TO YOUR OWN FOLDER ON GOOGLE DRIVE\n",
    "else:\n",
    "  data_path = \"./data/\"  ## DO NOT MODIFY THIS LINE. CHANGING THIS LINE WOULD RESULT IN FAIL OF AUTOGRADER TESTS\n",
    "\n",
    "assert os.path.exists(data_path), \"Change data_path to a valid and existing file path in your google drive!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWKbjCm2FiXZ"
   },
   "source": [
    "### Load the provided files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3305,
     "status": "ok",
     "timestamp": 1733688067094,
     "user": {
      "displayName": "Huy Tran",
      "userId": "16988764098922800274"
     },
     "user_tz": 300
    },
    "id": "WDnungmrkPsj",
    "outputId": "b7a55951-f971-472c-b02e-e9bf328a8959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"sciam.json.gz\" already exists, not downloading.\n",
      "File \"P3train.trecrun\" already exists, not downloading.\n",
      "File \"P3train.tsv\" already exists, not downloading.\n",
      "File \"P3eval.tsv\" already exists, not downloading.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_file(file_path: str, gz_zip: bool = True, is_json: bool=False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load strings from the text or gzip file. Remember to strip newline if necessary!\n",
    "\n",
    "    Args:\n",
    "        file_path: the location of file we want to analyze on.\n",
    "        gz_zip: true if the file is gzipped, false otherwise\n",
    "\n",
    "    Returns: an array of string loaded from the text or gzip file.\n",
    "    \"\"\"\n",
    "    webloc = \"https://cs.umass.edu/~allan/cs446/\"\n",
    "\n",
    "\n",
    "    data_info = Path(data_path)\n",
    "    if not data_info.exists() or not data_info.is_dir():\n",
    "      print(f\"Google folder \\\"{data_path}\\\" is not present or not a folder. Nothing will work from here.\")\n",
    "      return []\n",
    "\n",
    "    local_google_drive_path = os.path.join(data_path,file_path)\n",
    "    local_file = Path(local_google_drive_path)\n",
    "    if local_file.is_file():\n",
    "        print(f\"File \\\"{file_path}\\\" already exists, not downloading.\")\n",
    "    else:\n",
    "        print(f\"Cannot find \\\"{file_path}\\\" so downloading it\")\n",
    "        urllib.request.urlretrieve(webloc + file_path, local_google_drive_path)\n",
    "        print(\"Done\")\n",
    "\n",
    "    results = []\n",
    "    if not gz_zip:\n",
    "        f = open(local_google_drive_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    else:\n",
    "        # Read compressed file (opened in text mode since that's what we use)\n",
    "        f = gzip.open(local_google_drive_path, \"rt\", encoding=\"utf-8\")\n",
    "\n",
    "    # Pull in the contents of the file and return it\n",
    "    if is_json:\n",
    "      results = json.load(f)['corpus']\n",
    "    elif '.tsv' in file_path:\n",
    "      results = [line.strip(\"\\n\").split('\\t') for line in f.readlines() if line]\n",
    "    elif '.trecrun' in file_path:\n",
    "      results = [re.split(r'[ \\t]+', line.strip(\"\\n\")) for line in f.readlines() if line]\n",
    "    if f:\n",
    "      f.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# path to documents and stopwords\n",
    "documents_path = \"sciam.json.gz\" # path to gzip file that contains documents\n",
    "P3_train_path = \"P3train.tsv\" # Path to p1 train dataset\n",
    "P3_trecrun_path = \"P3train.trecrun\"  # path to the output trecrun\n",
    "P3_eval_path = \"P3eval.tsv\"  # path to the file that contains queries for P3 evaluation\n",
    "\n",
    "documents = load_file(documents_path, is_json=True)\n",
    "trecrun  = load_file(P3_trecrun_path, gz_zip = False)\n",
    "queries  = load_file(P3_train_path, gz_zip = False)\n",
    "eval_queries = load_file(P3_eval_path, gz_zip = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaMl5vRzIp8i"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset is stories from Scientific American issues from the 1800s. The data has been tokenized and stemmed (not stopped) and made available as sciam.json.gz. (You will probably want to look at it uncompressed, but be sure that your code handles the compressed version only.) The stories were extracted from 25 issues of the magazine. If you're curious, you can look at the original magazine issues at https://www.gutenberg.org/ebooks/bookshelf/256.\n",
    "\n",
    "This dataset has been preprocessed by stripping out punctuation and applying the Krovetz Stemmer (not Porter). No stopwords have been removed. Note that this means that the tokenization and stemming steps (as in P1) have been performed and, because stemming has already happened, that _you will not use a stopword list_. (Plus, of course, this will allow you to support queries with conjunction words -- e.g., the three words “clumps and keys” in order!)\n",
    "\n",
    "An example story is below. The text element is a actually single line in the input file but is presented wrapped here for ease of presentation.  The tokens are always separated by space characters, meaning that you will essentially be using a space tokenizer!\n",
    "\n",
    "```\n",
    "{  \"article\" : \"8952\",\n",
    "   \"storyID\" : \"8952-id_18\",\n",
    "   \"storynum\" : \"id_18\",\n",
    "   \"url\" : \"https://www.gutenberg.org/cache/epub/8952/pg8952-images.html#id_18\",\n",
    "   \"text\" : \"invention patent in england by americacompile from the journal of\n",
    "   the commissioner of patentprovisional protection for six month3 201 sewing\n",
    "   machine h a house bridgeport conn november 4 1869 3 211 bore tool alexander\n",
    "   allen new york city november 5 1869 3 215 mode of and device foe secure\n",
    "   stair rod h uhry new york city november 6 1869 3 229 transportation of\n",
    "   letters parcel and other freight by atmospheric pressure and in apparatus\n",
    "   connected therewith a e beach stratford conn november 9 1869 3 303 reload\n",
    "   cartridge shell r j gatl indianapolis ind november 16 1869 3 342 wooden\n",
    "   pavement i hayward and j f paul boston mass november 20 1869 3 358 machinery\n",
    "   for distribute type o l brown boston mass november 20 1869 3 219 weigh\n",
    "   machine m kennedy new york city november 10 1869 3 260 bran duster w huntley\n",
    "   and a babcock silver creek n y november 12 1869 3 339 railway carriage e\n",
    "   robbin cincinnati ohio november 19 1869 3 341 revolving battery gun r j gatl\n",
    "   indianapolis ind nov 19 1869 3 360 sash fastener s l loomi south byron n y\n",
    "   november 20 1869 3 363 magnetic machine and magnet j burrough jr newark n j\n",
    "   november 20 1869  \"  \n",
    "}\n",
    "```\n",
    "\n",
    "This story is from article number 8952 published in Scientific American, Volume 22, No. 1, January 1, 1870. The story has an internal number of `id_18`: the storyID combines the article number and the story number to yield `8952-id_18`. The URL will get you to that story directly. If you scroll to the top you'll see the article's date.\n",
    "\n",
    "For purposes of P3, only the storyID and the text are important. You may find the others helpful, particularly the URL if you want to decipher some of these documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucmDVvJuFDU1"
   },
   "source": [
    "<a name=\"document-parsing\"></a>\n",
    "## 1. Parse the collection of documents in `document_fpath` to build an in-memory index.\n",
    "\n",
    "To implement the first component of `run_queries(document_fpath, query_fpath, trecrun_file)`, we want to read in the tokenized and stemmed document collection provided and build an inverted index. Here are some things you will probably want to take into account.\n",
    "\n",
    "- The documents are provided as a list of json dictionary objects.\n",
    "- Because of the pre-processing we have done for you, term vectors should be constructed by splitting the text field's value by spaces (the regex \\\\s+), and ignoring blank strings. You've already shown you can tokenize, stop, and stem, so we're making it easier for this project.\n",
    " - Note that the processing in this case did not squeeze apostrophes out: instead, they were treated as word separators. That means that the word `quaker's` has resulted in tokens `quaker` and also `s`. Similarly, you'll find `t` (from `don't`) and `ve` (from `I've`) and so on as tokens. You will handle them as if they were normal words: do not remove them! That way the phrase query `\"quaker s\"` (see Part II) should find the word `quaker's` (and also `quaker s'truth` if by some weird chance that appeared in the stories).\n",
    "- Note that the index includes Unicode characters, but you should not worry about them even if they create odd tokens. Tokens are whatever is separated by a whitespace.\n",
    "- You need to build a simple inverted index with count and positional information: a combination of Figure 5.4 (p.134) and Figure 5.5 (p.135) in the textbook. (Depending on how you store things, the count may come along for free with the position information.) While you may store your index to disk to enable reuse, it is not required: an in-memory only index is sufficient for the project. (Your decision may be made partly by how long it takes to process the full collection in your implementation. It should be fast.)\n",
    "\n",
    "- Here are some recommendations regarding the data structures you will use behind these functions to implement the indea. You can use whatever data structures you feel are appropriate to implement these functcions, however, you may find these ideas helpful. We will not explicitly test your data structures.\n",
    "  - We recommend having the data storage behind your inverted index being of a form similar to **Dict[str, List[Posting]]**. Other data structures are plausible but are unlikely to provide any particular value for this project.\n",
    " - You might define a **PostingList** class that provides the **List[Posting]** datatype. Mostly that would help abstract away the implementation of the list.\n",
    " - **Posting** might be a **DocId** and **Positions** pair, so that a dictionary is used to find inverted lists and the inverted list is a list of \"Postings.\" It might also include the count of postings in that document.\n",
    " - **Positions** might be a **List[int]** or **array** of integers\n",
    " - **For the sake of efficiency (string comparisons are slow)** and memory usage, we encourage you to encode document numbers as integers, not using the *storyID* textual field. Otherwise you will not be able to answer analysis questions about compression. However, note that you will still need to be able to convert a document number back into its corresponding *storyID* for the TREC run output file. For example: getStoryID(1) would be “8951-id_1” since that's the first story in *sciam.json.gz*.\n",
    "\n",
    "- We suggest you think ahead (see [processing queries](#query-processing) below) and ensure your index can access the vocabulary, term counts, document counts and other statistics that you will need to perform the query evaluation activities. Thinking this through now will save some headaches later.\n",
    "\n",
    " - You will be writing code to handle arbitrary (but simple) queries for phrases and terms. That means your code will benefit from being general in how it handles them. You will likely find it best to have a method that allows any number of terms and/or phrases and does not depend on knowing what those terms and/or phrases will be or even how many there will be. Here are a couple of tips or ideas that may or may not help your thinking:\n",
    "   - You will be implementing scoring functions and models that produce scores that are not just integers. Put some time into it now (read ahead!) to ensure that your inverted index provides everything that is needed for those operations -- e.g., the number of terms, the number of documents, and so on. What do you think you might need to store and where would it make sense to store it?\n",
    "   - Would it benefit you to define helper functions for processes such as counting phrases and terms?\n",
    "   - Can you implement terms as phrases of length 1? That is, have one function that handles phrases and single terms that doesn't care whether there are 1 or 32 terms in the phrase. That provides a cleaner abstraction.\n",
    "\n",
    "Below is the stub of a helper function that can be implemented to build and return an inverted index for `run_queries(document_fpath, query_fpath, trecrun_file)`. You are not required to use this, and it will not be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "KTdbdwisLUB3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "\n",
    "# postings can be doc id: term count, list[positions]\n",
    "# how inverted list should be implemented:\n",
    "#    term: Posting()\n",
    "#        -> where object posting can be defined as shown above\n",
    "# need to make it easy to get statistics, like term counts, document counts so on and so forth.\n",
    "# need to use json parsing library (python standard) to make parsing file easier\n",
    "# -> list of dictionaries i believe or something.\n",
    "\n",
    "\"\"\"\n",
    "a posting is mapping from a single term that gives information that:\n",
    "    -> total times it occurs in the entire corpus\n",
    "    -> a list of documents that the term occurs in\n",
    "        -> the positions of which the term occurs in that document\n",
    "        -> the number of time that term occurs in that document\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def load_json(file_path: str, is_json: bool) -> None:\n",
    "    f = gzip.open(file_path, \"rt\", encoding=\"utf-8\")\n",
    "\n",
    "    if is_json:\n",
    "        results = json.load(f)['corpus']\n",
    "    else:\n",
    "        results = [line.strip(\"\\n\") for line in f.readlines() if line]\n",
    "    \n",
    "    return results\n",
    "\n",
    "class Posting:\n",
    "    def __init__(self):\n",
    "        self.total_term_count = 0\n",
    "        self.doc_to_id = {}     # doc number: storyID\n",
    "        self.id_to_doc = {}     # storyID: doc number\n",
    "        self.doc_to_term = {}   # doc_id: [term_count, [positions], word_count]\n",
    "\n",
    "    def add_story_id(self, document_number: int, storyID: str) -> None:\n",
    "        self.doc_to_id[document_number] = storyID\n",
    "        self.id_to_doc[storyID] = document_number\n",
    "    \n",
    "    # doc_id will correspond to the document number\n",
    "    def add_doc_id_info(self, doc_id: int, term_count: int, positions: list[int], word_count: int) -> None:\n",
    "        if doc_id in self.doc_to_term.keys(): # if we want to add a new term and the doc_id already exists, we just want to update the information\n",
    "            new_count = term_count + self.doc_to_term[doc_id][0] # first position is term count\n",
    "            self.doc_to_term[doc_id][1].extend(positions)\n",
    "            self.doc_to_term.update({doc_id: [new_count, self.doc_to_term[doc_id][1], word_count]})\n",
    "\n",
    "        else:\n",
    "            self.doc_to_term[doc_id] = [term_count, positions, word_count]\n",
    "    \n",
    "    def get_document_occurence(self): # returns the number of documents that this term occurs in\n",
    "        return len(self.doc_to_term.keys())\n",
    " \n",
    "def build_inverted_index(file):\n",
    "    inverted_index = {}\n",
    "    d = load_json(file, is_json=True)\n",
    "    for i, dictionary in enumerate(d):\n",
    "        document = dictionary.get('text')\n",
    "\n",
    "        positions = {}\n",
    "        document_word_lst = [word for word in document.split() if word]\n",
    "        \n",
    "        D = len(document_word_lst) # number of words in document\n",
    "        for position, word in enumerate(document_word_lst): # obtains word: list(type: int, contains: zero-indexed positions)\n",
    "            if word:\n",
    "                positions.setdefault(word, []).append(position)\n",
    "            \n",
    "        for word, positions in positions.items():\n",
    "            if word not in inverted_index:\n",
    "                new_posting = Posting()\n",
    "                new_posting.add_story_id(i, dictionary.get(\"storyID\"))\n",
    "                new_posting.add_doc_id_info(i, len(positions), positions, D)\n",
    "                inverted_index[word] = new_posting\n",
    "            else:\n",
    "                inverted_index[word].add_story_id(i, dictionary.get(\"storyID\"))\n",
    "                inverted_index[word].add_doc_id_info(i, len(positions), positions, D)\n",
    "\n",
    "            inverted_index[word].total_term_count += len(positions)\n",
    "    return inverted_index\n",
    "\n",
    "# Example usage\n",
    "# d = load_json(\"./data/sciam.json.gz\", is_json=True)\n",
    "inverted_index = build_inverted_index(\"./data/sciam.json.gz\")\n",
    "# print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1fS3x8rJhsC"
   },
   "source": [
    "### 1.1 Debugging the Index\n",
    "\n",
    "We recommend that you consider a helper method to print out some information to help you debug your index. There are other ways to do this, but we're setting things up to ensure you can do it this way if you like. We will not test this function or even verify that it exists. (Though please make sure the cell does not trigger an error.)\n",
    "\n",
    "For example, you might print out some information that will be helpful for verifying your index overall for some array of terms. It could first print the number of documents, the number of unique terms that are in the collection, and the total number of term occurrences. Then, for each term listed, it could print out one line with the term, the number of documents containing that term, and the total number of occurrences of that term.\n",
    "\n",
    "Here is some sample output that you should get for this data using this collection, but since we will not actually test your code (it is for debugging only), the actual format does not matter.\n",
    "\n",
    "```\n",
    "debug_inverted_index(inverted_index, [\"and\",\"the\", \"elephant\",\"science\"], showTerms=False)\n",
    "\n",
    "976 docs, 27217 terms, 1185999 occurrences\n",
    "and 956 docs, 34896 occurrences\n",
    "the 966 docs, 96151 occurrences\n",
    "elephant 7 docs, 18 occurrences\n",
    "science 112 docs, 251 occurrences\n",
    "\n",
    "```\n",
    "\n",
    "You might also consider creating something that prints out parts of the inverted list so you can see what's there. For example, you might have the argument “showTerms” that prints out the inverted list for the listed terms. For example, you might show something like this:\n",
    "\n",
    "\n",
    "```\n",
    "debug_inverted_index(inverted_index, [\"elephant\", \"science\"], showTerms=True)\n",
    "\n",
    "**** elephant - 7 docs, 18 occurrences\n",
    "  at {8952-id_12 [259]} {38481-art31 [317, 438, 464]} {38482-art25 [3, 24, 47]} {8391-id_13 [5, 35, 80, 106, 350, 1014, 1225, 1466]} {8504-id_30 [38]} {24322-art09 [647]} {15193-art17 [396]}\n",
    "**** science - 112 docs, 251 occurrences\n",
    "  at {8951-id_9 [35]} {8951-id_21 [171]} {8952-id_15 [1561]} {8952-id_30 [40, 69, 120]} {8952-id_31 [5, 108, 134, 172, 406, 429, 485, 493, 559, 623, 739, 780, 956, 1063, 1072, 1126, 1183, 1391, 1684, 1693, 1941, 2015, 2172, 2222, 2302, 2343, 2392, 2404, 2551, 2564]} {8952-id_33 [196, 244, 524]} {8952-id_44 [112]} {8952-id_46 [18]} {8952-id_69 [9595, 15195]} {19180-art22 [1645]} {19180-art28 [90, 152]} {19180-art33 [131, 389]} {19180-art35 [277, 302, 565, 701, 852, 926]} {19180-art41 [152]} {19180-art53 [4613, 4625, 10221, 11314, 11326, 17166]} {19406-art48 [329, 554, 862, 1537]} {19406-art31 [5]} {19406-art44 [502, 726]} {19406-art68 [892]}\n",
    "```\n",
    "\n",
    "(For reasons of space, the output for `science` is truncated and the output for `and` as well as `the` is not shown at all.)\n",
    "\n",
    "Reminder, providing these debug functions is entirely optional. The autograder will not test whether you have any code that does that. However, you are likely to find something like this helpful as you debug your code -- e.g., it gives you a list of the documents that your query processing should return for simple one-word queries. Also, it's fair for the teaching staff to ask you for something comparable when you post a question that appears like it might be the result of incorrect indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732528523982,
     "user": {
      "displayName": "Debrup Das",
      "userId": "04906306871166104749"
     },
     "user_tz": 300
    },
    "id": "B5PUlV88JBRu",
    "outputId": "0c5dd916-5350-4801-e391-d9900ef22ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 34896 occurrences 956 docs\n",
      "the 96151 occurrences 966 docs\n",
      "elephant 18 occurrences 7 docs\n",
      "science 251 occurrences 112 docs\n"
     ]
    }
   ],
   "source": [
    "def debug_inverted_index(inverted_index, terms, showTerms=True):\n",
    "    \"\"\"\n",
    "        Print out debugging information for the index\n",
    "\n",
    "        Args:\n",
    "            inverted_index: the inverted index data structure.\n",
    "            terms: an array of strings, representing the terms to debug\n",
    "            showTerms: defaults to true, will print out inverted list instead of # of docs and occurrences for each term\n",
    "\n",
    "        Returns: n/a\n",
    "      \"\"\"\n",
    "    for term in terms:\n",
    "        if term in inverted_index.keys():\n",
    "            posting = inverted_index[term]\n",
    "            print(term, posting.total_term_count, \"occurrences\", posting.get_document_occurence(), \"docs\")\n",
    "\n",
    "# Example usage\n",
    "debug_inverted_index(inverted_index, [\"and\", \"the\", \"elephant\",\"science\"], showTerms=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGAxufCzKO4h"
   },
   "source": [
    "<a name=\"query-processing\"></a>\n",
    "## 2. Read and process a list of queries from `query_fpath`\n",
    "\n",
    "The second component of `run_queries()` requires you to run a sequence of queries that are provided in *P3_eval.tsv*. You will want to have read the appropriate parts of Chapter 7 in the text. Recall that *sciam.json.gz* is the set of documents you index and *P3_eval.tsv* is a list of queries to run. We are also providing *P3train.tsv* and *P3train.trecrun* which are a set of sample queries and their expected output, respectively.\n",
    "\n",
    "The list of queries (in *P3train.tsv* and in *P3_eval.tsv*) comprises lines with at least three fields separated by tabs (tsv = tab separated values) in the format:\n",
    "\n",
    "> queryType queryName wordPhrase1 wordPhrase2 … wordPhraseN\n",
    "\n",
    "where:\n",
    "\n",
    "- *queryType* is one of the following, indicating what type of query you will be processing (case does not matter, so AND, and, And, aNd, and so on, are all the same):\n",
    " - **AND** means a Boolean query that is the AND of all of the *wordPhrases* listed.\n",
    " - **OR** means a Boolean query that is the OR of all of the *wordPhrases* listed.\n",
    " - **QL** means you will run a query likelihood algorithm with the *wordPhrases* as the query. QL is discussed starting in 7.3.1 of the textbook. You are asked to implement the version that uses Dirichlet smoothing, discussion of which starts on page 258 of both versions of the textbook. Use μ=300.\n",
    " - **BM25** means you will use the BM25 algorithm with the *wordPhrases* being the query. BM25 scoring is described starting on page 250 of the PDF or printed textbook. Please use these values for your scoring: k_1=1.8, k2=5, b=0.75.\n",
    " - **TF** means you will calculate the raw term frequency of *wordPhrase* for each story in which it appears. Only wordPhrase1 (one phrase or word) would be present for TF. Rank of documents will be determined based on value of term frequency. Do not include documents that do not contain the phrase in the trecrun output file.\n",
    " - **DF** means you will calculate the document frequency of each *wordPhrase* listed (yes, even if it is a phrase). There will be N seperate lines for N wordPhrases. Your output (shown later) in trecrun file will show the *wordPhrase* as if it were a document and the DF as the score. Rank will be ignored (just output i as rank for WordPhrasei).\n",
    "- *queryName* is a provided name for this query (e.g., \"query1\" or \"12354\" or \"smelt\"). The first column of every output line will be this *queryName* (for this query).\n",
    "- *wordPhraseK* are the words/phrases that you need to find using your index.\n",
    " - If there are multiple words (separated by spaces) in a *WordPhrase*, it means a phrase and that those words must occur adjacent in an article and in that given order. Although phrases are often indicated by quotation marks elsewhere, they are _not_ used here.\n",
    " - Note that there will always be at least one *wordPhrase* and there is no theoretical limit on the value of N.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "565kqXUNDAmI"
   },
   "source": [
    "###**Handling phrases**\n",
    "\n",
    "If a *wordPhrase* contains one or more spaces, it is a phrase. (Recall that the *wordPhrases* are separated by tabs, so there can be spaces within them.) A phrase requires a match with two or more adjacent words in each document.\n",
    "\n",
    "For QL and BM25 (and DF), you need to know the number of times a phrase occurs in a document. (For Boolean you just have to know that number is at least one for a given document.) To do that, you'll have to actually do the equivalent of running that phrase as if it were a query. Remember that you are only interested in phrases, where the words occur next to each other in the order given; you do not need to handle things like “within 5 words” or “in the same sentence.”\n",
    "\n",
    "TF and DF and essentially index-probing \"query\" types. For DF, you must allow multiple *wordPhrase* occurrences as in all of the others and expect them to be phrases or single words. For TF, only 1 WordPhrase will be present in the query. For TF, you will be scoring and ranking the documents by the TF count (with ties broken appropriately). For DF, your output will display the *wordPhrase* where the document would normally be, the DF value where the score would be, and the rank will be ignored (still output i for WordPhrase i).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w42CvQFDGEs"
   },
   "source": [
    "<a name=\"output-formatting\"></a>\n",
    "###**Output format**\n",
    "\n",
    "The output of your program will be a file called *output.trecrun* containing a number of lines per query in *P3_eval.tsv*. You are going to use the \"TREC run format\" as follows. For each query, you will display its ranked list using exactly six columns per line. White space (spaces or tabs) is used to separate columns. The width of the columns in the format is not important, but it is important to have exactly six columns per line with at least one space between the columns (note that no queryName or storyID contains a space). The format is,\n",
    "\n",
    "> *queryName* skip *storyID* rank score *yourUsername*\n",
    "\n",
    "where\n",
    "\n",
    "- the first column is the *queryName* from the file *P3train.tsv*\n",
    "- the second column is currently unused and should always be `skip`.\n",
    "- the third column is the value of the *storyID* JSON field of this ranked document. (For the DF query type, it should be the *wordPhrase*.)\n",
    "- the fourth column is the rank of the document retrieved. You will list things sorted by rank within the query, with the highest score having rank 1 and there being no ties (even though there might be ties in the scores; break ties by sorting by *storyID*). The rank will be ignored for the DF type, though you must provide a value.)\n",
    "- the fifth column shows the score (floating point with 4 places of decimal) that generated the ranking. This score must be in descending (non-increasing) order. The score for a Boolean query will always be 1.0000. The score for TF and DF query types should be the term frequency or document frequency, respectively.\n",
    "- the sixth column is called the \"run tag\" and is traditionally a unique identifier for you and for the method used. In this case, just use your SPIRE username (e.g., \"allan\" for the professor). Please do not turn in results with \"username\" (or \"allan\") as the identifier.\n",
    "\n",
    "\n",
    "Here is an example of what this might look like (the information is made up; see sample output for real results). It aligns the columns to make them easier to read; you do not have to do that. For example,\n",
    "```\n",
    "Qbm   skip 8951-id_20           1    1.0000   username\n",
    "Qbm   skip 19406-art03          2    0.5000   username\n",
    "Qbm   skip 19412-art13          3    0.2000   username\n",
    "Qql   skip 38481-artnq13        1    0.3333   username\n",
    "Qql   skip 21081-article44-11   2    0.2505   username\n",
    "…\n",
    "Qtf   skip 13443-elec1          1    7.0000   username\n",
    "Qtf   skip 38481-artnq13        2    1.0000   username\n",
    "Qdf1  skip {WordPhrase1}        1    8.0000   username\n",
    "Qdf1  skip {WordPhrase2}        2    5.0000   username\n",
    "…\n",
    "```\n",
    "**Note that all queries are printed to _output.trecrun_** (the filename passed into the run_queries function), with all matching stories per query.\n",
    "\n",
    "(1) For QL, BM25, and TF, a story matches a ranked query if it contains at least one of the query terms (TF has only 1 term);\n",
    "\n",
    "(2) It matches a Boolean query AND, OR if it satisfies the Boolean expression.\n",
    "\n",
    "(3) If any QL, BM25, AND, OR, TF, DF query happens to retrieve zero documents (or DF is zero for a WordPhrase), there will be no lines in the output file for that query (no lines for the WordPhrase for DF). If any TF query has 0 term frequency for some document, that document should be omitted from your output. Any term frequency greater than zero for any document should otherwise be printed out to *output.trecrun*.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**NOTE**: If any *wordPhrase* in a DF query has 0 document frequency, there will be no line in the output file for that *wordPhrase*. Also note that for the DF measure, the *wordPhrase* instances are independent of each other: they are not processed as a group as they are for the other items. Just output one line per *wordPhrase* word or phrase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khK8rRdi8IMT"
   },
   "source": [
    "###2.1 Running TF and DF queries\n",
    "\n",
    "TF and DF calculate a score that is intended to reflect the raw term frequency or document frequency, respectively. That score produces the ranking.\n",
    "\n",
    "For TF, the score for a document is the raw term frequency, or the number of times a *wordPhrase* occurs in the document. For any TF query, we will calculate the raw term frequency across all documents, and rank according to the raw term frequency. Ties should be broken according to the *storyID* string (see for AND and OR below in Section 2.2). For example, '21081-article1' comes before '8952-id_63'. If the TF is 0 for any document, do not list that document in the output. **For TF, return all documents that contain at least one occurrence of the term**. Documents without the term will not be listed.\n",
    "\n",
    "For DF, the output is a bit different. The \"storyID\" field will be replaced by the *wordPhrase*, the score will be the document frequency for that *wordPhrase*, and the rank will be ignored. If the DF is 0 for the given *wordPhrase*, we will not print anything in *output.trecrun* for that *wordPhrase*. **NOTE: If the _wordPhrase_ is a phrase -- i.e., it contains spaces -- replace all of the spaces with underscores.** That is because the trecrun file format does not allow spaces in docids and we want to be consistent with that format. So `DF qname united<tab>states<tab>united states` woudl result in one line for `united`, another for `states`, and one for the phrase that would look liked `united_states`. Note that if the query were actually `DF qname united_states` the output would look the same (`united_states`) but the DF count would almost certainly be different.\n",
    "\n",
    "Below are sample helper methods that you can use to help implement TF and DF query processing for `index_run()`. However, you are not required to use this method, and will not be graded on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "-QDfSLmhi2kI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def filter_postings(wordPhrase: str) -> dict[str: dict[int: list[int]]]: # word: dict[doc_id: positions]\n",
    "    \"\"\"\n",
    "    wordPhrase can either be a phrase (multiple words with at least one space, i.e., \"united states\")\n",
    "    or can either be a single term, \"lincoln\"\n",
    "    \"\"\"\n",
    "    word_info = {} # {(word: str, doc_id: int): positions: list[int]}\n",
    "    split_phrase = [word for word in wordPhrase.split() if word != \"\"]\n",
    "    doc_id_lst = []\n",
    "    if all(True if word in inverted_index.keys() else False for word in split_phrase):\n",
    "\n",
    "        for word in split_phrase:\n",
    "            tmp = [] # holds doc_ids of which the current word of iteration occurs in\n",
    "            for doc_id in inverted_index[word].doc_to_term.keys():\n",
    "                tmp.append(doc_id)\n",
    "            doc_id_lst.append(tmp) # list of lists, each individually represents what docs contain which term, where the indice is the key\n",
    "\n",
    "        combined_lst = list(set.intersection(*map(set, doc_id_lst)))\n",
    "        \n",
    "        if len(combined_lst) == 0:\n",
    "            return None # if intersection results elements sharing no common documents, then the phrase cannot be returned properly.\n",
    "\n",
    "        big_dict = {}\n",
    "        for word in split_phrase:\n",
    "            for doc_id, items in inverted_index[word].doc_to_term.items():\n",
    "                if doc_id in combined_lst:\n",
    "                    word_info[doc_id] = items[1]\n",
    "            big_dict[word] = word_info\n",
    "            word_info = {}\n",
    "        return big_dict\n",
    "    \n",
    "\n",
    "def count_phrase_occurrence(doc_id: int, phrase: list[str], filtered_postings: dict) -> int:\n",
    "    pos_dict = {\n",
    "        word: list(filtered_postings[word][doc_id])\n",
    "        for word in phrase if word in filtered_postings.keys() and doc_id in filtered_postings[word].keys()\n",
    "    }\n",
    "    if len(pos_dict) < len(phrase):\n",
    "        return 0  \n",
    "\n",
    "    phrase_occurrences = 0\n",
    "    for start_pos in pos_dict[phrase[0]]:\n",
    "        if all(start_pos + i in pos_dict[phrase[i]] for i in range(1, len(phrase))):\n",
    "            phrase_occurrences += 1\n",
    "    return phrase_occurrences\n",
    "\n",
    "def check_adj(postings, word_lst, doc_id):\n",
    "    \"\"\"\n",
    "    postings: dict[word: dict[doc_id: positions -> list[int] ]]\n",
    "    word_lst: word seperated list, returned from ele.split(), i.e., [\"hello\", \"world\"]\n",
    "    doc_id: document ids that contain every element in word_lst\n",
    "    \"\"\"\n",
    "    if postings is None:\n",
    "        return False\n",
    "\n",
    "    pos = [postings.get(word, {}).get(doc_id, []) for word in word_lst]\n",
    "\n",
    "    for i in range(1, len(pos)):\n",
    "        prev_word_positions = pos[i - 1]\n",
    "        current_word_positions = pos[i]\n",
    "        if not prev_word_positions or not current_word_positions:\n",
    "            return False\n",
    "        if not any(p + 1 in current_word_positions for p in prev_word_positions):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_total_word_count(inverted_index):\n",
    "    total = 0\n",
    "    for posting in inverted_index.values():\n",
    "        total += posting.total_term_count\n",
    "    return total\n",
    "\n",
    "def tf(inverted_index, doc_id, term):\n",
    "    \"\"\"\n",
    "      Calculate the number of times a term or phrase appears in a document\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          doc_id: the document we want to calculate the term frequency for.\n",
    "          term: the term or phrase we want to calculate the frequency for.\n",
    "\n",
    "      Returns: an integer representing the number of times the term appears in doc_id.\n",
    "    \"\"\"\n",
    "\n",
    "    term_frequency = 0\n",
    "    term_lst = [w for w in term.split() if w]\n",
    "    \n",
    "    if len(term_lst) == 1:\n",
    "        if doc_id in inverted_index[term].doc_to_term.keys(): \n",
    "            return inverted_index[term].doc_to_term[doc_id][0] # doc_to_term -> doc_id: [term_count, positions, word_count(in doc)]\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    elif len(term_lst) > 1:\n",
    "        filtered_postings = filter_postings(term)\n",
    "        if filtered_postings is not None:\n",
    "            if check_adj(filtered_postings, term_lst, doc_id):\n",
    "                return count_phrase_occurrence(doc_id, term_lst, filtered_postings)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    return term_frequency\n",
    "v = tf(inverted_index, 975, \"united states\")\n",
    "print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "eg5JE8xtjcIE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    }
   ],
   "source": [
    "def df(inverted_index, wordPhrase):\n",
    "    \"\"\"\n",
    "      Calculate the number of documents a term or phrase appears in\n",
    "      NOTE that this sample only handles one wordPhrase at at time and so only returns\n",
    "      a single number. This is primarily to highlight that it operates differently\n",
    "      than the others that are ranking documents.\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          wordPhrase: the term or phrase we want to calculate the frequency for.\n",
    "\n",
    "      Returns: an integer representing the number of documents a term or phrase appears in.\n",
    "    \"\"\"\n",
    "\n",
    "    doc_freq = 0\n",
    "    term_lst = wordPhrase.split()\n",
    "    if len(term_lst) == 1:\n",
    "        return inverted_index[wordPhrase].get_document_occurence()\n",
    "    \n",
    "    elif len(term_lst) > 1:\n",
    "        filtered_postings = filter_postings(wordPhrase)\n",
    "        for doc_id in filtered_postings[term_lst[0]].keys():\n",
    "            if check_adj(filtered_postings, term_lst, doc_id):\n",
    "                doc_freq += 1\n",
    "\n",
    "    return doc_freq\n",
    "test = \"united states\"\n",
    "# test = \"hospital\"\n",
    "t = df(inverted_index, test)\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "ow4xur7dj3XA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258\n"
     ]
    }
   ],
   "source": [
    "def collection_frequency(inverted_index, term):\n",
    "    \"\"\"\n",
    "      Calculate the number of times a term or phrase appears in the corpus\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          term: the term or phrase we want to calculate the frequency for.\n",
    "\n",
    "      Returns: an integer representing the number of times the term appears in the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    collection_freq = 0\n",
    "\n",
    "    term_lst = [t for t in term.split() if t]\n",
    "    if len(term_lst) == 1:\n",
    "        return inverted_index[term].total_term_count if term in inverted_index.keys() else collection_freq\n",
    "    \n",
    "    elif len(term_lst) > 1:\n",
    "        filtered_postings = filter_postings(term)\n",
    "        for doc_id in filtered_postings[term_lst[0]].keys():\n",
    "            if check_adj(filtered_postings, term_lst, doc_id):\n",
    "                collection_freq += count_phrase_occurrence(doc_id, term_lst, filtered_postings)\n",
    "\n",
    "    return collection_freq\n",
    "t = \"united states\"\n",
    "f = collection_frequency(inverted_index, t)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfSED4v6KR37"
   },
   "source": [
    "### 2.2 Running Boolean queries\n",
    "Your program will be expected to handle queries that find words, phrases (two or more adjacent words), or combinations of both. The queries will return the complete set of *storyID*s that match the Boolean query. Obviously there will never be more than the total number of stories in the collection.\n",
    "\n",
    "There are no scores other than 1.0 (match) or 0.0 (doesn't match) calculated, so sort the matching documents (all of which have a score of 1.0) by the *storyID* string (as a string, ignoring that it might look like a number in some ways -- i.e., “11-x” comes before “2-x”. '21081-article1' comes before '8952-id_63') and print out the list that way, increasing the rank from 1 through the number that match.\n",
    "\n",
    "Below are sample helper methods that you can use to help implement boolean query processing for `run_queries()`. However, remember that you are not required to use this method, and will not be graded on the method itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "yfigiQaEOF9w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('14990-art03', 1.0), ('15193-art16', 1.0), ('15050-VIII_1', 1.0)] \n",
      " 3\n"
     ]
    }
   ],
   "source": [
    "def intersect(inverted_index, wordPhrases):\n",
    "    \"\"\"\n",
    "      Evaluate an AND boolean query over all wordPhrases in the query\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          wordPhrases: a list of words or phrases to search for.\n",
    "\n",
    "      Returns: an array of tuples, where the first value is the doc_id, and the second is the score\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    wordPhrase_lst = [phrase for phrase in wordPhrases.split(\"\\t\") if phrase]  \n",
    "    filtered_docs = None\n",
    "\n",
    "    for phrase in wordPhrase_lst:\n",
    "        if len(phrase) > 1 and \" \" in phrase: # multiple words in this phrase\n",
    "            word_separated_lst = [word for word in phrase.split() if word]\n",
    "            filtered_postings = filter_postings(phrase)\n",
    "\n",
    "            if filtered_postings is None:\n",
    "                return results\n",
    "\n",
    "            phrase_docs = set(filtered_postings[word_separated_lst[0]].keys())  # get doc_ids for the first word, doesnt matter which one since the rest must occur in the doc of the first word\n",
    "            valid_docs = []\n",
    "\n",
    "            for doc_id in phrase_docs:\n",
    "                if check_adj(filtered_postings, word_separated_lst, doc_id):  # check adjacency\n",
    "                    valid_docs.append(doc_id)\n",
    "\n",
    "            if filtered_docs is None:\n",
    "                filtered_docs = set(valid_docs)\n",
    "            else:\n",
    "                filtered_docs &= set(valid_docs)  # Global AND across phrases\n",
    "\n",
    "        else:  # phrase is a single word\n",
    "            filtered_postings = filter_postings(phrase)\n",
    "            if filtered_postings is None:\n",
    "                return results\n",
    "            valid_docs = set(filtered_postings[phrase].keys())\n",
    "\n",
    "            if filtered_docs is None:\n",
    "                filtered_docs = valid_docs\n",
    "            else:\n",
    "                filtered_docs &= valid_docs\n",
    "\n",
    "    if filtered_docs:\n",
    "        for doc_id in filtered_docs:\n",
    "            # Assuming the first word in the first phrase maps to doc_id -> id\n",
    "            results.append((inverted_index[wordPhrase_lst[0].split()[0]].doc_to_id[doc_id], 1.0))\n",
    "\n",
    "    return results\n",
    "\n",
    "test_phrase = \"pony\"\n",
    "# test_phrase = \"plant in sleep rooms the following\\tplant in sleep following\"\n",
    "# test_phrase = \"improvement\"\n",
    "t = intersect(inverted_index, test_phrase)\n",
    "print(t, \"\\n\", len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "GAT1Op5YOG8F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('8951-id_6', 1.0), ('16353-art04', 1.0)] \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "def union(inverted_index, wordPhrases):\n",
    "    \"\"\"\n",
    "      Evaluate an OR boolean query over all wordPhrases in the query\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          wordPhrases: a list of words or phrases to search for.\n",
    "\n",
    "      Returns: a set of tuples, where the first value is the doc_id, and the second is the score\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    wordPhrase_lst = [word for word in wordPhrases.split(\"\\t\") if word]\n",
    "    for phrase in wordPhrase_lst:\n",
    "        if len(phrase) > 1 and \" \" in phrase:  # word Phrase contains multiple words and at least a single space\n",
    "\n",
    "            filtered_postings = filter_postings(phrase) # get documents that contain all phrases, dict[word: dict[doc_id: positions]]\n",
    "            word_seperated_lst = [word for word in phrase.split() if word != \"\"] # word seperated list\n",
    "            doc_id_lst = [doc_id for doc_id in filtered_postings[word_seperated_lst[0]].keys()] # filtered_postings only contains doc_ids that contain all elements of the phrase\n",
    "\n",
    "            for doc_id in doc_id_lst:\n",
    "                if check_adj(filtered_postings, word_seperated_lst, doc_id):\n",
    "                    results.append((inverted_index[word_seperated_lst[0]].doc_to_id[doc_id], 1.0))\n",
    "        else:\n",
    "            filtered_postings = filter_postings(phrase) # phrase is a single word\n",
    "            results = [(inverted_index[phrase].doc_to_id[doc_id], 1.0) for doc_id in filtered_postings[phrase].keys()]\n",
    "\n",
    "    return results\n",
    "\n",
    "test_phrase = \"united states\\tlincoln\"\n",
    "# test_phrase = \"plant in sleep rooms the following\\tplant in sleep rooms the\"\n",
    "# test_phrase = \"improvement\"\n",
    "t = union(inverted_index, test_phrase)\n",
    "print(t, \"\\n\", len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erwaOuSsKjUU"
   },
   "source": [
    "### 2.3 Running QL and BM25 queries\n",
    "\n",
    "The QL and BM25 approaches calculate a score that is intended to reflect the probability of relevance in some way. That score produces the ranking.\n",
    "\n",
    "For QL, the score is the product of a number of small numbers (probabilities), which runs the risk of creating arithmetic underflow. Represent probabilities as the log (**natural log**) of the probabilities and sum them. That means that the score for these ranking runs will be negative, since the log of a number less than one (e.g., a probability) will be negative. Sorting in reverse order by the negative scores will put higher probabilities at the start of the list as you want.\n",
    "\n",
    "For QL and BM25, **only print documents at ranks 1 through 100, inclusive**, unless the query matches fewer than 100 stories, **in which case list everything retrieved** (that is, any story that includes at least one of the query terms). If there are documents with the same score, break the tie by sorting those by the storyID textual field's value (as a string, ignoring that it looks like a number in some ways -- i.e., “11-x” comes before “2-x”). Similarly, '21081-article1' comes before '8952-id_63'.\n",
    "\n",
    "Below are sample helper methods that you can use to help implement QL and BM25 query processing for `run_queries()`. They copy the constant values that were listed above for ease of reference. However, you are not required to use this method, and will not be graded on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "gd-uU6eGOKkO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('11649-id_2', -4.963147658527492)\n",
      "1 ('8952-id_40', -5.244936921786223)\n",
      "2 ('11648-id_8', -5.572067865732792)\n",
      "3 ('18265-art09', -6.173732601770033)\n",
      "4 ('16948-art15', -6.189170378057936)\n",
      "5 ('15889-art01', -6.237600111117887)\n",
      "6 ('8952-id_5', -6.265220492967906)\n",
      "7 ('15889-art18', -6.377277952740295)\n",
      "8 ('38480-art08', -6.588351091864685)\n",
      "9 ('16360-art10', -6.608585651896489)\n",
      "10 ('11344-id_27', -6.64628376078785)\n",
      "11 ('11734-id_7', -7.065960332969618)\n",
      "12 ('18265-art15', -7.86342233213508)\n",
      "13 ('11344-id_30', -7.866806430119321)\n",
      "14 ('15050-VII_2', -7.907275723328385)\n",
      "15 ('15193-art16', -7.988917756201962)\n",
      "16 ('15417-art12', -8.207132236056523)\n",
      "17 ('15889-art17', -8.271270035890396)\n",
      "18 ('8504-id_28', -8.322776233464158)\n",
      "19 ('8504-id_25', -8.524030756908251)\n",
      "20 ('16353-art18', -8.639649757961186)\n",
      "21 ('15193-art02', -8.743900472575646)\n",
      "22 ('15708-ref16', -8.905474233493573)\n",
      "23 ('8951-id_1', -9.567255407572533)\n",
      "24 ('8951-id_2', -9.567255407572533)\n",
      "25 ('8951-id_3', -9.567255407572533)\n",
      "26 ('8951-id_4', -9.567255407572533)\n",
      "27 ('8951-id_5', -9.567255407572533)\n",
      "28 ('8951-id_6', -9.567255407572533)\n",
      "29 ('8951-id_7', -9.567255407572533)\n",
      "30 ('8951-id_8', -9.567255407572533)\n",
      "31 ('8951-id_9', -9.567255407572533)\n",
      "32 ('8951-id_10', -9.567255407572533)\n",
      "33 ('8951-id_11', -9.567255407572533)\n",
      "34 ('8951-id_12', -9.567255407572533)\n",
      "35 ('8951-id_15', -9.567255407572533)\n",
      "36 ('8951-id_13', -9.567255407572533)\n",
      "37 ('8951-id_14', -9.567255407572533)\n",
      "38 ('8951-id_16', -9.567255407572533)\n",
      "39 ('8951-id_17', -9.567255407572533)\n",
      "40 ('8951-id_18', -9.567255407572533)\n",
      "41 ('8951-id_19', -9.567255407572533)\n",
      "42 ('8951-id_20', -9.567255407572533)\n",
      "43 ('8951-id_21', -9.567255407572533)\n",
      "44 ('8951-id_22', -9.567255407572533)\n",
      "45 ('8951-id_23', -9.567255407572533)\n",
      "46 ('8951-id_24', -9.567255407572533)\n",
      "47 ('8951-id_25', -9.567255407572533)\n",
      "48 ('8951-id_26', -9.567255407572533)\n",
      "49 ('8951-id_27', -9.567255407572533)\n",
      "50 ('8952-id_1', -9.567255407572533)\n",
      "51 ('8952-id_2', -9.567255407572533)\n",
      "52 ('8952-id_3', -9.567255407572533)\n",
      "53 ('8952-id_4', -9.567255407572533)\n",
      "54 ('8952-id_6', -9.567255407572533)\n",
      "55 ('8952-id_7', -9.567255407572533)\n",
      "56 ('8952-id_8', -9.567255407572533)\n",
      "57 ('8952-id_9', -9.567255407572533)\n",
      "58 ('8952-id_10', -9.567255407572533)\n",
      "59 ('8952-id_11', -9.567255407572533)\n",
      "60 ('8952-id_12', -9.567255407572533)\n",
      "61 ('8952-id_13', -9.567255407572533)\n",
      "62 ('8952-id_14', -9.567255407572533)\n",
      "63 ('8952-id_15', -9.567255407572533)\n",
      "64 ('8952-id_16', -9.567255407572533)\n",
      "65 ('8952-id_17', -9.567255407572533)\n",
      "66 ('8952-id_18', -9.567255407572533)\n",
      "67 ('8952-id_19', -9.567255407572533)\n",
      "68 ('8952-id_20', -9.567255407572533)\n",
      "69 ('8952-id_21', -9.567255407572533)\n",
      "70 ('8952-id_22', -9.567255407572533)\n",
      "71 ('8952-id_23', -9.567255407572533)\n",
      "72 ('8952-id_24', -9.567255407572533)\n",
      "73 ('8952-id_29', -9.567255407572533)\n",
      "74 ('8952-id_27', -9.567255407572533)\n",
      "75 ('8952-id_28', -9.567255407572533)\n",
      "76 ('8952-id_26', -9.567255407572533)\n",
      "77 ('8952-id_25', -9.567255407572533)\n",
      "78 ('8952-id_30', -9.567255407572533)\n",
      "79 ('8952-id_31', -9.567255407572533)\n",
      "80 ('8952-id_32', -9.567255407572533)\n",
      "81 ('8952-id_33', -9.567255407572533)\n",
      "82 ('8952-id_34', -9.567255407572533)\n",
      "83 ('8952-id_35', -9.567255407572533)\n",
      "84 ('8952-id_36', -9.567255407572533)\n",
      "85 ('8952-id_37', -9.567255407572533)\n",
      "86 ('8952-id_38', -9.567255407572533)\n",
      "87 ('8952-id_39', -9.567255407572533)\n",
      "88 ('8952-id_41', -9.567255407572533)\n",
      "89 ('8952-id_42', -9.567255407572533)\n",
      "90 ('8952-id_43', -9.567255407572533)\n",
      "91 ('8952-id_44', -9.567255407572533)\n",
      "92 ('8952-id_45', -9.567255407572533)\n",
      "93 ('8952-id_46', -9.567255407572533)\n",
      "94 ('8952-id_47', -9.567255407572533)\n",
      "95 ('8952-id_48', -9.567255407572533)\n",
      "96 ('8952-id_49', -9.567255407572533)\n",
      "97 ('8952-id_50', -9.567255407572533)\n",
      "98 ('8952-id_51', -9.567255407572533)\n",
      "99 ('8952-id_52', -9.567255407572533)\n",
      "100 ('8952-id_53', -9.567255407572533)\n",
      "101 ('8952-id_54', -9.567255407572533)\n",
      "102 ('8952-id_55', -9.567255407572533)\n",
      "103 ('8952-id_56', -9.567255407572533)\n",
      "104 ('8952-id_57', -9.567255407572533)\n",
      "105 ('8952-id_58', -9.567255407572533)\n",
      "106 ('8952-id_59', -9.567255407572533)\n",
      "107 ('8952-id_60', -9.567255407572533)\n",
      "108 ('8952-id_61', -9.567255407572533)\n",
      "109 ('8952-id_62', -9.567255407572533)\n",
      "110 ('8952-id_63', -9.567255407572533)\n",
      "111 ('8952-id_64', -9.567255407572533)\n",
      "112 ('8952-id_65', -9.567255407572533)\n",
      "113 ('8952-id_66', -9.567255407572533)\n",
      "114 ('8952-id_67', -9.567255407572533)\n",
      "115 ('8952-id_68', -9.567255407572533)\n",
      "116 ('19180-art01', -9.567255407572533)\n",
      "117 ('19180-art02', -9.567255407572533)\n",
      "118 ('19180-art03', -9.567255407572533)\n",
      "119 ('19180-art04', -9.567255407572533)\n",
      "120 ('19180-art05', -9.567255407572533)\n",
      "121 ('19180-art06', -9.567255407572533)\n",
      "122 ('19180-art07', -9.567255407572533)\n",
      "123 ('19180-art08', -9.567255407572533)\n",
      "124 ('19180-art09', -9.567255407572533)\n",
      "125 ('19180-art10', -9.567255407572533)\n",
      "126 ('19180-art11', -9.567255407572533)\n",
      "127 ('19180-art12', -9.567255407572533)\n",
      "128 ('19180-art13', -9.567255407572533)\n",
      "129 ('19180-art14', -9.567255407572533)\n",
      "130 ('19180-art15', -9.567255407572533)\n",
      "131 ('19180-art16', -9.567255407572533)\n",
      "132 ('19180-art17', -9.567255407572533)\n",
      "133 ('19180-art18', -9.567255407572533)\n",
      "134 ('19180-art19', -9.567255407572533)\n",
      "135 ('19180-art20', -9.567255407572533)\n",
      "136 ('19180-art21', -9.567255407572533)\n",
      "137 ('19180-art22', -9.567255407572533)\n",
      "138 ('19180-art23', -9.567255407572533)\n",
      "139 ('19180-art24', -9.567255407572533)\n",
      "140 ('19180-art25', -9.567255407572533)\n",
      "141 ('19180-art26', -9.567255407572533)\n",
      "142 ('19180-art27', -9.567255407572533)\n",
      "143 ('19180-art28', -9.567255407572533)\n",
      "144 ('19180-art29', -9.567255407572533)\n",
      "145 ('19180-art30', -9.567255407572533)\n",
      "146 ('19180-art31', -9.567255407572533)\n",
      "147 ('19180-art32', -9.567255407572533)\n",
      "148 ('19180-art33', -9.567255407572533)\n",
      "149 ('19180-art34', -9.567255407572533)\n",
      "150 ('19180-art35', -9.567255407572533)\n",
      "151 ('19180-art36', -9.567255407572533)\n",
      "152 ('19180-art37', -9.567255407572533)\n",
      "153 ('19180-art38', -9.567255407572533)\n",
      "154 ('19180-art39', -9.567255407572533)\n",
      "155 ('19180-art40', -9.567255407572533)\n",
      "156 ('19180-art41', -9.567255407572533)\n",
      "157 ('19180-art42', -9.567255407572533)\n",
      "158 ('19180-art43', -9.567255407572533)\n",
      "159 ('19180-art44', -9.567255407572533)\n",
      "160 ('19180-art45', -9.567255407572533)\n",
      "161 ('19180-art46', -9.567255407572533)\n",
      "162 ('19180-art47', -9.567255407572533)\n",
      "163 ('19180-art48', -9.567255407572533)\n",
      "164 ('19180-art49', -9.567255407572533)\n",
      "165 ('19180-art50', -9.567255407572533)\n",
      "166 ('19180-art51', -9.567255407572533)\n",
      "167 ('19180-art52', -9.567255407572533)\n",
      "168 ('19180-art53', -9.567255407572533)\n",
      "169 ('19406-art17', -9.567255407572533)\n",
      "170 ('19406-art55', -9.567255407572533)\n",
      "171 ('19406-art48', -9.567255407572533)\n",
      "172 ('19406-art03', -9.567255407572533)\n",
      "173 ('19406-art06', -9.567255407572533)\n",
      "174 ('19406-art49', -9.567255407572533)\n",
      "175 ('19406-art30', -9.567255407572533)\n",
      "176 ('19406-art54', -9.567255407572533)\n",
      "177 ('19406-art66', -9.567255407572533)\n",
      "178 ('19406-art67', -9.567255407572533)\n",
      "179 ('19406-art64', -9.567255407572533)\n",
      "180 ('19406-art69', -9.567255407572533)\n",
      "181 ('19406-art10', -9.567255407572533)\n",
      "182 ('19406-art29', -9.567255407572533)\n",
      "183 ('19406-art53', -9.567255407572533)\n",
      "184 ('19406-art42', -9.567255407572533)\n",
      "185 ('19406-art47', -9.567255407572533)\n",
      "186 ('19406-art62', -9.567255407572533)\n",
      "187 ('19406-art23', -9.567255407572533)\n",
      "188 ('19406-art43', -9.567255407572533)\n",
      "189 ('19406-art31', -9.567255407572533)\n",
      "190 ('19406-art01', -9.567255407572533)\n",
      "191 ('19406-art50', -9.567255407572533)\n",
      "192 ('19406-art33', -9.567255407572533)\n",
      "193 ('19406-art44', -9.567255407572533)\n",
      "194 ('19406-art32', -9.567255407572533)\n",
      "195 ('19406-art34', -9.567255407572533)\n",
      "196 ('19406-art35', -9.567255407572533)\n",
      "197 ('19406-art40', -9.567255407572533)\n",
      "198 ('19406-art26', -9.567255407572533)\n",
      "199 ('19406-art15', -9.567255407572533)\n",
      "200 ('19406-art52', -9.567255407572533)\n",
      "201 ('19406-art13', -9.567255407572533)\n",
      "202 ('19406-art57', -9.567255407572533)\n",
      "203 ('19406-art36', -9.567255407572533)\n",
      "204 ('19406-art60', -9.567255407572533)\n",
      "205 ('19406-art63', -9.567255407572533)\n",
      "206 ('19406-art61', -9.567255407572533)\n",
      "207 ('19406-art68', -9.567255407572533)\n",
      "208 ('19406-art07', -9.567255407572533)\n",
      "209 ('19406-art18', -9.567255407572533)\n",
      "210 ('19406-art24', -9.567255407572533)\n",
      "211 ('19406-art05', -9.567255407572533)\n",
      "212 ('19406-art56', -9.567255407572533)\n",
      "213 ('19406-art58', -9.567255407572533)\n",
      "214 ('19406-art37', -9.567255407572533)\n",
      "215 ('19406-art39', -9.567255407572533)\n",
      "216 ('19406-art45', -9.567255407572533)\n",
      "217 ('19406-art09', -9.567255407572533)\n",
      "218 ('19406-art51', -9.567255407572533)\n",
      "219 ('19406-art08', -9.567255407572533)\n",
      "220 ('19406-art25', -9.567255407572533)\n",
      "221 ('19406-art70', -9.567255407572533)\n",
      "222 ('19406-art21', -9.567255407572533)\n",
      "223 ('19406-art28', -9.567255407572533)\n",
      "224 ('19406-art12', -9.567255407572533)\n",
      "225 ('19406-art19', -9.567255407572533)\n",
      "226 ('19406-art38', -9.567255407572533)\n",
      "227 ('19406-art59', -9.567255407572533)\n",
      "228 ('19406-art16', -9.567255407572533)\n",
      "229 ('19406-art20', -9.567255407572533)\n",
      "230 ('19406-art65', -9.567255407572533)\n",
      "231 ('19406-art22', -9.567255407572533)\n",
      "232 ('19406-art04', -9.567255407572533)\n",
      "233 ('19406-art27', -9.567255407572533)\n",
      "234 ('19406-art11', -9.567255407572533)\n",
      "235 ('19406-art14', -9.567255407572533)\n",
      "236 ('19406-art02', -9.567255407572533)\n",
      "237 ('19406-art46', -9.567255407572533)\n",
      "238 ('38481-art1', -9.567255407572533)\n",
      "239 ('38481-art2', -9.567255407572533)\n",
      "240 ('38481-art3', -9.567255407572533)\n",
      "241 ('38481-art4', -9.567255407572533)\n",
      "242 ('38481-art5', -9.567255407572533)\n",
      "243 ('38481-art7', -9.567255407572533)\n",
      "244 ('38481-art8', -9.567255407572533)\n",
      "245 ('38481-art9', -9.567255407572533)\n",
      "246 ('38481-art10', -9.567255407572533)\n",
      "247 ('38481-art11', -9.567255407572533)\n",
      "248 ('38481-art12', -9.567255407572533)\n",
      "249 ('38481-art13', -9.567255407572533)\n",
      "250 ('38481-art14', -9.567255407572533)\n",
      "251 ('38481-art15', -9.567255407572533)\n",
      "252 ('38481-art16', -9.567255407572533)\n",
      "253 ('38481-art17', -9.567255407572533)\n",
      "254 ('38481-art18', -9.567255407572533)\n",
      "255 ('38481-art19', -9.567255407572533)\n",
      "256 ('38481-art20', -9.567255407572533)\n",
      "257 ('38481-art21', -9.567255407572533)\n",
      "258 ('38481-art22', -9.567255407572533)\n",
      "259 ('38481-art23', -9.567255407572533)\n",
      "260 ('38481-art24', -9.567255407572533)\n",
      "261 ('38481-art25', -9.567255407572533)\n",
      "262 ('38481-art26', -9.567255407572533)\n",
      "263 ('38481-art27', -9.567255407572533)\n",
      "264 ('38481-art28', -9.567255407572533)\n",
      "265 ('38481-art29', -9.567255407572533)\n",
      "266 ('38481-art30', -9.567255407572533)\n",
      "267 ('38481-art31', -9.567255407572533)\n",
      "268 ('38481-art32', -9.567255407572533)\n",
      "269 ('38481-art33', -9.567255407572533)\n",
      "270 ('38481-art34', -9.567255407572533)\n",
      "271 ('38481-art35', -9.567255407572533)\n",
      "272 ('38481-art36', -9.567255407572533)\n",
      "273 ('38481-art37', -9.567255407572533)\n",
      "274 ('38481-art38', -9.567255407572533)\n",
      "275 ('38481-art39', -9.567255407572533)\n",
      "276 ('38481-art40', -9.567255407572533)\n",
      "277 ('38481-art43', -9.567255407572533)\n",
      "278 ('38481-art41', -9.567255407572533)\n",
      "279 ('38481-art42', -9.567255407572533)\n",
      "280 ('38481-art44', -9.567255407572533)\n",
      "281 ('38481-art45', -9.567255407572533)\n",
      "282 ('38481-artnq2', -9.567255407572533)\n",
      "283 ('38481-artnq5', -9.567255407572533)\n",
      "284 ('38481-artnq13', -9.567255407572533)\n",
      "285 ('38481-artnq20', -9.567255407572533)\n",
      "286 ('38481-artnq21', -9.567255407572533)\n",
      "287 ('38481-artnq22', -9.567255407572533)\n",
      "288 ('38481-artnq24', -9.567255407572533)\n",
      "289 ('38481-artnq30', -9.567255407572533)\n",
      "290 ('38481-artnq32', -9.567255407572533)\n",
      "291 ('38481-artnq33', -9.567255407572533)\n",
      "292 ('38481-artnq35', -9.567255407572533)\n",
      "293 ('38481-artnq39', -9.567255407572533)\n",
      "294 ('38481-artnq40', -9.567255407572533)\n",
      "295 ('38481-art', -9.567255407572533)\n",
      "296 ('38481-art46', -9.567255407572533)\n",
      "297 ('38480-art01', -9.567255407572533)\n",
      "298 ('38480-art02', -9.567255407572533)\n",
      "299 ('38480-art03', -9.567255407572533)\n",
      "300 ('38480-art04', -9.567255407572533)\n",
      "301 ('38480-art05', -9.567255407572533)\n",
      "302 ('38480-art06', -9.567255407572533)\n",
      "303 ('38480-art07', -9.567255407572533)\n",
      "304 ('38480-art09', -9.567255407572533)\n",
      "305 ('38480-art10', -9.567255407572533)\n",
      "306 ('38480-art11', -9.567255407572533)\n",
      "307 ('38480-art12', -9.567255407572533)\n",
      "308 ('38480-art13', -9.567255407572533)\n",
      "309 ('38480-art14', -9.567255407572533)\n",
      "310 ('38480-art15', -9.567255407572533)\n",
      "311 ('38480-art16', -9.567255407572533)\n",
      "312 ('38480-art17', -9.567255407572533)\n",
      "313 ('38480-art18', -9.567255407572533)\n",
      "314 ('38480-art19', -9.567255407572533)\n",
      "315 ('38480-art20', -9.567255407572533)\n",
      "316 ('38480-art21', -9.567255407572533)\n",
      "317 ('38480-art22', -9.567255407572533)\n",
      "318 ('38480-art24', -9.567255407572533)\n",
      "319 ('38480-art25', -9.567255407572533)\n",
      "320 ('38480-art26', -9.567255407572533)\n",
      "321 ('38480-art27', -9.567255407572533)\n",
      "322 ('38480-art28', -9.567255407572533)\n",
      "323 ('38480-art29', -9.567255407572533)\n",
      "324 ('38480-art30', -9.567255407572533)\n",
      "325 ('38480-art31', -9.567255407572533)\n",
      "326 ('38480-art32', -9.567255407572533)\n",
      "327 ('38480-art33', -9.567255407572533)\n",
      "328 ('38480-art34', -9.567255407572533)\n",
      "329 ('38480-art35', -9.567255407572533)\n",
      "330 ('38480-art36', -9.567255407572533)\n",
      "331 ('38480-art37', -9.567255407572533)\n",
      "332 ('38480-art38', -9.567255407572533)\n",
      "333 ('38480-art39', -9.567255407572533)\n",
      "334 ('38480-art40', -9.567255407572533)\n",
      "335 ('38480-art41', -9.567255407572533)\n",
      "336 ('38480-art42', -9.567255407572533)\n",
      "337 ('38480-art43', -9.567255407572533)\n",
      "338 ('38480-art44', -9.567255407572533)\n",
      "339 ('38480-art45', -9.567255407572533)\n",
      "340 ('38480-artnq02', -9.567255407572533)\n",
      "341 ('38480-artnq03', -9.567255407572533)\n",
      "342 ('38480-artnq04', -9.567255407572533)\n",
      "343 ('38480-artnq05', -9.567255407572533)\n",
      "344 ('38480-artnq06', -9.567255407572533)\n",
      "345 ('38480-artnq08', -9.567255407572533)\n",
      "346 ('38480-artnq14', -9.567255407572533)\n",
      "347 ('38480-artnq16', -9.567255407572533)\n",
      "348 ('38480-artnq19', -9.567255407572533)\n",
      "349 ('38480-artnq36', -9.567255407572533)\n",
      "350 ('38480-artnq42', -9.567255407572533)\n",
      "351 ('38480-artnq43', -9.567255407572533)\n",
      "352 ('38480-artnq48', -9.567255407572533)\n",
      "353 ('38480-artnq51', -9.567255407572533)\n",
      "354 ('38480-artnq55', -9.567255407572533)\n",
      "355 ('38480-artnq57', -9.567255407572533)\n",
      "356 ('38482-art1', -9.567255407572533)\n",
      "357 ('38482-art2', -9.567255407572533)\n",
      "358 ('38482-art3', -9.567255407572533)\n",
      "359 ('38482-art3ar', -9.567255407572533)\n",
      "360 ('38482-art4', -9.567255407572533)\n",
      "361 ('38482-art5', -9.567255407572533)\n",
      "362 ('38482-art6', -9.567255407572533)\n",
      "363 ('38482-art7', -9.567255407572533)\n",
      "364 ('38482-art8', -9.567255407572533)\n",
      "365 ('38482-art9', -9.567255407572533)\n",
      "366 ('38482-art10', -9.567255407572533)\n",
      "367 ('38482-art3a', -9.567255407572533)\n",
      "368 ('38482-art11', -9.567255407572533)\n",
      "369 ('38482-art12', -9.567255407572533)\n",
      "370 ('38482-art13', -9.567255407572533)\n",
      "371 ('38482-art14', -9.567255407572533)\n",
      "372 ('38482-art15', -9.567255407572533)\n",
      "373 ('38482-art16', -9.567255407572533)\n",
      "374 ('38482-art17', -9.567255407572533)\n",
      "375 ('38482-art18', -9.567255407572533)\n",
      "376 ('38482-art19', -9.567255407572533)\n",
      "377 ('38482-art20', -9.567255407572533)\n",
      "378 ('38482-art21', -9.567255407572533)\n",
      "379 ('38482-art22', -9.567255407572533)\n",
      "380 ('38482-art23', -9.567255407572533)\n",
      "381 ('38482-art24', -9.567255407572533)\n",
      "382 ('38482-art25', -9.567255407572533)\n",
      "383 ('38482-art26', -9.567255407572533)\n",
      "384 ('38482-art27', -9.567255407572533)\n",
      "385 ('38482-art28', -9.567255407572533)\n",
      "386 ('38482-art29', -9.567255407572533)\n",
      "387 ('38482-art30', -9.567255407572533)\n",
      "388 ('38482-art31', -9.567255407572533)\n",
      "389 ('38482-art32', -9.567255407572533)\n",
      "390 ('38482-art33', -9.567255407572533)\n",
      "391 ('38482-art34', -9.567255407572533)\n",
      "392 ('38482-art35', -9.567255407572533)\n",
      "393 ('38482-art36', -9.567255407572533)\n",
      "394 ('38482-art37', -9.567255407572533)\n",
      "395 ('38482-art38', -9.567255407572533)\n",
      "396 ('38482-art39', -9.567255407572533)\n",
      "397 ('38482-art40', -9.567255407572533)\n",
      "398 ('38482-art41', -9.567255407572533)\n",
      "399 ('38482-art42', -9.567255407572533)\n",
      "400 ('21081-article1', -9.567255407572533)\n",
      "401 ('21081-article3', -9.567255407572533)\n",
      "402 ('21081-article4', -9.567255407572533)\n",
      "403 ('21081-article5', -9.567255407572533)\n",
      "404 ('21081-article6', -9.567255407572533)\n",
      "405 ('21081-article7', -9.567255407572533)\n",
      "406 ('21081-article8', -9.567255407572533)\n",
      "407 ('21081-article9', -9.567255407572533)\n",
      "408 ('21081-article10', -9.567255407572533)\n",
      "409 ('21081-article11', -9.567255407572533)\n",
      "410 ('21081-article12', -9.567255407572533)\n",
      "411 ('21081-article13', -9.567255407572533)\n",
      "412 ('21081-article14', -9.567255407572533)\n",
      "413 ('21081-article15', -9.567255407572533)\n",
      "414 ('21081-article16', -9.567255407572533)\n",
      "415 ('21081-article17', -9.567255407572533)\n",
      "416 ('21081-article18', -9.567255407572533)\n",
      "417 ('21081-article19', -9.567255407572533)\n",
      "418 ('21081-article20', -9.567255407572533)\n",
      "419 ('21081-article21', -9.567255407572533)\n",
      "420 ('21081-article22', -9.567255407572533)\n",
      "421 ('21081-article23', -9.567255407572533)\n",
      "422 ('21081-article24', -9.567255407572533)\n",
      "423 ('21081-article25', -9.567255407572533)\n",
      "424 ('21081-article26', -9.567255407572533)\n",
      "425 ('21081-article27', -9.567255407572533)\n",
      "426 ('21081-article28', -9.567255407572533)\n",
      "427 ('21081-article29', -9.567255407572533)\n",
      "428 ('21081-article30', -9.567255407572533)\n",
      "429 ('21081-article32', -9.567255407572533)\n",
      "430 ('21081-article33', -9.567255407572533)\n",
      "431 ('21081-article34', -9.567255407572533)\n",
      "432 ('21081-article36', -9.567255407572533)\n",
      "433 ('21081-article37', -9.567255407572533)\n",
      "434 ('21081-article38', -9.567255407572533)\n",
      "435 ('21081-article39', -9.567255407572533)\n",
      "436 ('21081-article40', -9.567255407572533)\n",
      "437 ('21081-article41', -9.567255407572533)\n",
      "438 ('21081-article44', -9.567255407572533)\n",
      "439 ('21081-article44-1', -9.567255407572533)\n",
      "440 ('21081-article44-2', -9.567255407572533)\n",
      "441 ('21081-article44-3', -9.567255407572533)\n",
      "442 ('21081-article44-4', -9.567255407572533)\n",
      "443 ('21081-article44-5', -9.567255407572533)\n",
      "444 ('21081-article44-6', -9.567255407572533)\n",
      "445 ('21081-article44-7', -9.567255407572533)\n",
      "446 ('21081-article44-8', -9.567255407572533)\n",
      "447 ('21081-article44-9', -9.567255407572533)\n",
      "448 ('21081-article44-10', -9.567255407572533)\n",
      "449 ('21081-article44-11', -9.567255407572533)\n",
      "450 ('21081-article44-12', -9.567255407572533)\n",
      "451 ('21081-article44-13', -9.567255407572533)\n",
      "452 ('21081-article44-14', -9.567255407572533)\n",
      "453 ('21081-article44-15', -9.567255407572533)\n",
      "454 ('21081-article44-16', -9.567255407572533)\n",
      "455 ('21081-article44-17', -9.567255407572533)\n",
      "456 ('21081-article44-18', -9.567255407572533)\n",
      "457 ('21081-article44-19', -9.567255407572533)\n",
      "458 ('21081-article44-20', -9.567255407572533)\n",
      "459 ('21081-article44-21', -9.567255407572533)\n",
      "460 ('21081-article44-22', -9.567255407572533)\n",
      "461 ('21081-article44-23', -9.567255407572533)\n",
      "462 ('21081-article44-24', -9.567255407572533)\n",
      "463 ('21081-article44-25', -9.567255407572533)\n",
      "464 ('21081-article44-26', -9.567255407572533)\n",
      "465 ('21081-article44-27', -9.567255407572533)\n",
      "466 ('21081-article44-28', -9.567255407572533)\n",
      "467 ('21081-article44-29', -9.567255407572533)\n",
      "468 ('21081-article44-30', -9.567255407572533)\n",
      "469 ('8391-id_4', -9.567255407572533)\n",
      "470 ('8391-id_5', -9.567255407572533)\n",
      "471 ('8391-id_6', -9.567255407572533)\n",
      "472 ('8391-id_7', -9.567255407572533)\n",
      "473 ('8391-id_8', -9.567255407572533)\n",
      "474 ('8391-id_21', -9.567255407572533)\n",
      "475 ('8391-id_9', -9.567255407572533)\n",
      "476 ('8391-id_10', -9.567255407572533)\n",
      "477 ('8391-id_11', -9.567255407572533)\n",
      "478 ('8391-id_12', -9.567255407572533)\n",
      "479 ('8391-id_13', -9.567255407572533)\n",
      "480 ('8391-id_14', -9.567255407572533)\n",
      "481 ('8391-id_15', -9.567255407572533)\n",
      "482 ('8391-id_16', -9.567255407572533)\n",
      "483 ('8391-id_17', -9.567255407572533)\n",
      "484 ('8391-id_18', -9.567255407572533)\n",
      "485 ('8391-id_19', -9.567255407572533)\n",
      "486 ('8391-id_20', -9.567255407572533)\n",
      "487 ('8391-id_1', -9.567255407572533)\n",
      "488 ('8391-id_2', -9.567255407572533)\n",
      "489 ('8391-id_3', -9.567255407572533)\n",
      "490 ('8391-id_33', -9.567255407572533)\n",
      "491 ('8391-id_22', -9.567255407572533)\n",
      "492 ('8391-id_23', -9.567255407572533)\n",
      "493 ('8391-id_24', -9.567255407572533)\n",
      "494 ('8391-id_25', -9.567255407572533)\n",
      "495 ('8391-id_26', -9.567255407572533)\n",
      "496 ('8391-id_27', -9.567255407572533)\n",
      "497 ('8391-id_28', -9.567255407572533)\n",
      "498 ('8391-id_29', -9.567255407572533)\n",
      "499 ('8391-id_30', -9.567255407572533)\n",
      "500 ('8391-id_31', -9.567255407572533)\n",
      "501 ('8391-id_32', -9.567255407572533)\n",
      "502 ('8408-id_38', -9.567255407572533)\n",
      "503 ('8408-id_36', -9.567255407572533)\n",
      "504 ('8408-id_22', -9.567255407572533)\n",
      "505 ('8408-id_23', -9.567255407572533)\n",
      "506 ('8408-id_24', -9.567255407572533)\n",
      "507 ('8408-id_25', -9.567255407572533)\n",
      "508 ('8408-id_1', -9.567255407572533)\n",
      "509 ('8408-id_2', -9.567255407572533)\n",
      "510 ('8408-id_3', -9.567255407572533)\n",
      "511 ('8408-id_4', -9.567255407572533)\n",
      "512 ('8408-id_5', -9.567255407572533)\n",
      "513 ('8408-id_6', -9.567255407572533)\n",
      "514 ('8408-id_7', -9.567255407572533)\n",
      "515 ('8408-id_8', -9.567255407572533)\n",
      "516 ('8408-id_9', -9.567255407572533)\n",
      "517 ('8408-id_10', -9.567255407572533)\n",
      "518 ('8408-id_11', -9.567255407572533)\n",
      "519 ('8408-id_12', -9.567255407572533)\n",
      "520 ('8408-id_37', -9.567255407572533)\n",
      "521 ('8408-id_13', -9.567255407572533)\n",
      "522 ('8408-id_14', -9.567255407572533)\n",
      "523 ('8408-id_15', -9.567255407572533)\n",
      "524 ('8408-id_16', -9.567255407572533)\n",
      "525 ('8408-id_26', -9.567255407572533)\n",
      "526 ('8408-id_31', -9.567255407572533)\n",
      "527 ('8408-id_17', -9.567255407572533)\n",
      "528 ('8408-id_18', -9.567255407572533)\n",
      "529 ('8408-id_19', -9.567255407572533)\n",
      "530 ('8408-id_20', -9.567255407572533)\n",
      "531 ('8408-id_21', -9.567255407572533)\n",
      "532 ('8408-id_27', -9.567255407572533)\n",
      "533 ('8408-id_28', -9.567255407572533)\n",
      "534 ('8408-id_29', -9.567255407572533)\n",
      "535 ('8408-id_30', -9.567255407572533)\n",
      "536 ('8408-id_32', -9.567255407572533)\n",
      "537 ('8408-id_33', -9.567255407572533)\n",
      "538 ('8408-id_34', -9.567255407572533)\n",
      "539 ('8408-id_35', -9.567255407572533)\n",
      "540 ('8504-id_31', -9.567255407572533)\n",
      "541 ('8504-id_26', -9.567255407572533)\n",
      "542 ('8504-id_29', -9.567255407572533)\n",
      "543 ('8504-id_30', -9.567255407572533)\n",
      "544 ('8504-id_1', -9.567255407572533)\n",
      "545 ('8504-id_2', -9.567255407572533)\n",
      "546 ('8504-id_3', -9.567255407572533)\n",
      "547 ('8504-id_4', -9.567255407572533)\n",
      "548 ('8504-id_5', -9.567255407572533)\n",
      "549 ('8504-id_6', -9.567255407572533)\n",
      "550 ('8504-id_22', -9.567255407572533)\n",
      "551 ('8504-id_23', -9.567255407572533)\n",
      "552 ('8504-id_24', -9.567255407572533)\n",
      "553 ('8504-id_7', -9.567255407572533)\n",
      "554 ('8504-id_8', -9.567255407572533)\n",
      "555 ('8504-id_9', -9.567255407572533)\n",
      "556 ('8504-id_10', -9.567255407572533)\n",
      "557 ('8504-id_11', -9.567255407572533)\n",
      "558 ('8504-id_12', -9.567255407572533)\n",
      "559 ('8504-id_13', -9.567255407572533)\n",
      "560 ('8504-id_14', -9.567255407572533)\n",
      "561 ('8504-id_15', -9.567255407572533)\n",
      "562 ('8504-id_16', -9.567255407572533)\n",
      "563 ('8504-id_17', -9.567255407572533)\n",
      "564 ('8504-id_18', -9.567255407572533)\n",
      "565 ('8504-id_19', -9.567255407572533)\n",
      "566 ('8504-id_20', -9.567255407572533)\n",
      "567 ('8504-id_21', -9.567255407572533)\n",
      "568 ('8504-id_32', -9.567255407572533)\n",
      "569 ('8504-id_33', -9.567255407572533)\n",
      "570 ('15417-art01', -9.567255407572533)\n",
      "571 ('15417-art06', -9.567255407572533)\n",
      "572 ('15417-art02', -9.567255407572533)\n",
      "573 ('15417-art03', -9.567255407572533)\n",
      "574 ('15417-art04', -9.567255407572533)\n",
      "575 ('15417-art07', -9.567255407572533)\n",
      "576 ('15417-art08', -9.567255407572533)\n",
      "577 ('15417-art05', -9.567255407572533)\n",
      "578 ('15417-art14', -9.567255407572533)\n",
      "579 ('15417-art15', -9.567255407572533)\n",
      "580 ('15417-art16', -9.567255407572533)\n",
      "581 ('15417-art09', -9.567255407572533)\n",
      "582 ('15417-art10', -9.567255407572533)\n",
      "583 ('15417-art28', -9.567255407572533)\n",
      "584 ('15417-art29', -9.567255407572533)\n",
      "585 ('15417-art17', -9.567255407572533)\n",
      "586 ('15417-art11', -9.567255407572533)\n",
      "587 ('15417-art18', -9.567255407572533)\n",
      "588 ('15417-art19', -9.567255407572533)\n",
      "589 ('15417-art12-a', -9.567255407572533)\n",
      "590 ('15417-art13', -9.567255407572533)\n",
      "591 ('15417-art25', -9.567255407572533)\n",
      "592 ('15417-art30', -9.567255407572533)\n",
      "593 ('15417-art31', -9.567255407572533)\n",
      "594 ('15417-art20', -9.567255407572533)\n",
      "595 ('15417-art21', -9.567255407572533)\n",
      "596 ('15417-art22', -9.567255407572533)\n",
      "597 ('15417-art23', -9.567255407572533)\n",
      "598 ('15417-art26', -9.567255407572533)\n",
      "599 ('15417-art27', -9.567255407572533)\n",
      "600 ('15417-art24', -9.567255407572533)\n",
      "601 ('8718-id_29', -9.567255407572533)\n",
      "602 ('8718-id_30', -9.567255407572533)\n",
      "603 ('8718-id_11', -9.567255407572533)\n",
      "604 ('8718-id_12', -9.567255407572533)\n",
      "605 ('8718-id_13', -9.567255407572533)\n",
      "606 ('8718-id_14', -9.567255407572533)\n",
      "607 ('8718-id_15', -9.567255407572533)\n",
      "608 ('8718-id_17', -9.567255407572533)\n",
      "609 ('8718-id_7', -9.567255407572533)\n",
      "610 ('8718-id_18', -9.567255407572533)\n",
      "611 ('8718-id_8', -9.567255407572533)\n",
      "612 ('8718-id_9', -9.567255407572533)\n",
      "613 ('8718-id_10', -9.567255407572533)\n",
      "614 ('8718-id_31', -9.567255407572533)\n",
      "615 ('8718-id_22', -9.567255407572533)\n",
      "616 ('8718-id_32', -9.567255407572533)\n",
      "617 ('8718-id_20', -9.567255407572533)\n",
      "618 ('8718-id_21', -9.567255407572533)\n",
      "619 ('8718-id_16', -9.567255407572533)\n",
      "620 ('8718-id_1', -9.567255407572533)\n",
      "621 ('8718-id_2', -9.567255407572533)\n",
      "622 ('8718-id_3', -9.567255407572533)\n",
      "623 ('8718-id_4', -9.567255407572533)\n",
      "624 ('8718-id_5', -9.567255407572533)\n",
      "625 ('8718-id_19', -9.567255407572533)\n",
      "626 ('8718-id_24', -9.567255407572533)\n",
      "627 ('8718-id_25', -9.567255407572533)\n",
      "628 ('8718-id_23', -9.567255407572533)\n",
      "629 ('8718-id_33', -9.567255407572533)\n",
      "630 ('8718-id_28', -9.567255407572533)\n",
      "631 ('8718-id_27', -9.567255407572533)\n",
      "632 ('8718-id_26', -9.567255407572533)\n",
      "633 ('8718-id_6', -9.567255407572533)\n",
      "634 ('11344-id_16', -9.567255407572533)\n",
      "635 ('11344-id_18', -9.567255407572533)\n",
      "636 ('11344-id_10', -9.567255407572533)\n",
      "637 ('11344-id_11', -9.567255407572533)\n",
      "638 ('11344-id_12', -9.567255407572533)\n",
      "639 ('11344-id_13', -9.567255407572533)\n",
      "640 ('11344-id_14', -9.567255407572533)\n",
      "641 ('11344-id_1', -9.567255407572533)\n",
      "642 ('11344-id_4', -9.567255407572533)\n",
      "643 ('11344-id_5', -9.567255407572533)\n",
      "644 ('11344-id_15', -9.567255407572533)\n",
      "645 ('11344-id_2', -9.567255407572533)\n",
      "646 ('11344-id_6', -9.567255407572533)\n",
      "647 ('11344-id_7', -9.567255407572533)\n",
      "648 ('11344-id_8', -9.567255407572533)\n",
      "649 ('11344-id_9', -9.567255407572533)\n",
      "650 ('11344-id_20', -9.567255407572533)\n",
      "651 ('11344-id_21', -9.567255407572533)\n",
      "652 ('11344-id_3', -9.567255407572533)\n",
      "653 ('11344-id_26', -9.567255407572533)\n",
      "654 ('11344-id_28', -9.567255407572533)\n",
      "655 ('11344-id_29', -9.567255407572533)\n",
      "656 ('11344-id_31', -9.567255407572533)\n",
      "657 ('11344-id_32', -9.567255407572533)\n",
      "658 ('11344-id_23', -9.567255407572533)\n",
      "659 ('11344-id_22', -9.567255407572533)\n",
      "660 ('11344-id_19', -9.567255407572533)\n",
      "661 ('11344-id_24', -9.567255407572533)\n",
      "662 ('11344-id_25', -9.567255407572533)\n",
      "663 ('16353-art08', -9.567255407572533)\n",
      "664 ('16353-art09', -9.567255407572533)\n",
      "665 ('16353-art23', -9.567255407572533)\n",
      "666 ('16353-art01', -9.567255407572533)\n",
      "667 ('16353-art02', -9.567255407572533)\n",
      "668 ('16353-art03', -9.567255407572533)\n",
      "669 ('16353-art04', -9.567255407572533)\n",
      "670 ('16353-art06', -9.567255407572533)\n",
      "671 ('16353-art05', -9.567255407572533)\n",
      "672 ('16353-art07', -9.567255407572533)\n",
      "673 ('16353-art19', -9.567255407572533)\n",
      "674 ('16353-art20', -9.567255407572533)\n",
      "675 ('16353-art10', -9.567255407572533)\n",
      "676 ('16353-art21', -9.567255407572533)\n",
      "677 ('16353-art22', -9.567255407572533)\n",
      "678 ('16353-art16', -9.567255407572533)\n",
      "679 ('16353-art17', -9.567255407572533)\n",
      "680 ('16353-art11', -9.567255407572533)\n",
      "681 ('16353-art12', -9.567255407572533)\n",
      "682 ('11734-id_3', -9.567255407572533)\n",
      "683 ('11734-id_4', -9.567255407572533)\n",
      "684 ('11734-id_5', -9.567255407572533)\n",
      "685 ('11734-id_6', -9.567255407572533)\n",
      "686 ('11734-id_8', -9.567255407572533)\n",
      "687 ('11734-id_9', -9.567255407572533)\n",
      "688 ('11734-id_10', -9.567255407572533)\n",
      "689 ('11734-id_11', -9.567255407572533)\n",
      "690 ('11734-id_12', -9.567255407572533)\n",
      "691 ('11734-id_16', -9.567255407572533)\n",
      "692 ('11734-id_17', -9.567255407572533)\n",
      "693 ('11734-id_13', -9.567255407572533)\n",
      "694 ('11734-id_14', -9.567255407572533)\n",
      "695 ('11734-id_23', -9.567255407572533)\n",
      "696 ('11734-id_18', -9.567255407572533)\n",
      "697 ('11734-id_1', -9.567255407572533)\n",
      "698 ('11734-id_24', -9.567255407572533)\n",
      "699 ('11734-id_25', -9.567255407572533)\n",
      "700 ('11734-id_20', -9.567255407572533)\n",
      "701 ('11734-id_21', -9.567255407572533)\n",
      "702 ('11734-id_22', -9.567255407572533)\n",
      "703 ('11734-id_2', -9.567255407572533)\n",
      "704 ('11734-id_15', -9.567255407572533)\n",
      "705 ('11383-id_3', -9.567255407572533)\n",
      "706 ('11383-id_4', -9.567255407572533)\n",
      "707 ('11383-id_5', -9.567255407572533)\n",
      "708 ('11383-id_6', -9.567255407572533)\n",
      "709 ('11383-id_7', -9.567255407572533)\n",
      "710 ('11383-id_8', -9.567255407572533)\n",
      "711 ('11383-id_9', -9.567255407572533)\n",
      "712 ('11383-id_10', -9.567255407572533)\n",
      "713 ('11383-id_13', -9.567255407572533)\n",
      "714 ('11383-id_14', -9.567255407572533)\n",
      "715 ('11383-id_15', -9.567255407572533)\n",
      "716 ('11383-id_16', -9.567255407572533)\n",
      "717 ('11383-id_11', -9.567255407572533)\n",
      "718 ('11383-id_1', -9.567255407572533)\n",
      "719 ('11383-id_12', -9.567255407572533)\n",
      "720 ('11383-id_2', -9.567255407572533)\n",
      "721 ('11383-id_19', -9.567255407572533)\n",
      "722 ('11383-id_17', -9.567255407572533)\n",
      "723 ('11383-id_18', -9.567255407572533)\n",
      "724 ('16360-art07', -9.567255407572533)\n",
      "725 ('16360-art16', -9.567255407572533)\n",
      "726 ('16360-art17', -9.567255407572533)\n",
      "727 ('16360-art06', -9.567255407572533)\n",
      "728 ('16360-art15', -9.567255407572533)\n",
      "729 ('16360-art08', -9.567255407572533)\n",
      "730 ('16360-art13', -9.567255407572533)\n",
      "731 ('16360-art21', -9.567255407572533)\n",
      "732 ('16360-art12', -9.567255407572533)\n",
      "733 ('16360-art25', -9.567255407572533)\n",
      "734 ('16360-art26', -9.567255407572533)\n",
      "735 ('16360-art18', -9.567255407572533)\n",
      "736 ('16360-art20', -9.567255407572533)\n",
      "737 ('16360-art14', -9.567255407572533)\n",
      "738 ('16360-art11', -9.567255407572533)\n",
      "739 ('16360-art27', -9.567255407572533)\n",
      "740 ('16360-art24', -9.567255407572533)\n",
      "741 ('16360-art23', -9.567255407572533)\n",
      "742 ('16360-art03', -9.567255407572533)\n",
      "743 ('16360-art19', -9.567255407572533)\n",
      "744 ('16360-art04', -9.567255407572533)\n",
      "745 ('16360-art05', -9.567255407572533)\n",
      "746 ('16360-art02', -9.567255407572533)\n",
      "747 ('16360-art01', -9.567255407572533)\n",
      "748 ('16360-art09', -9.567255407572533)\n",
      "749 ('11648-id_17', -9.567255407572533)\n",
      "750 ('11648-id_16', -9.567255407572533)\n",
      "751 ('11648-id_6', -9.567255407572533)\n",
      "752 ('11648-id_5', -9.567255407572533)\n",
      "753 ('11648-id_7', -9.567255407572533)\n",
      "754 ('11648-id_12', -9.567255407572533)\n",
      "755 ('11648-id_14', -9.567255407572533)\n",
      "756 ('11648-id_13', -9.567255407572533)\n",
      "757 ('11648-id_2', -9.567255407572533)\n",
      "758 ('11648-id_11', -9.567255407572533)\n",
      "759 ('11648-id_9', -9.567255407572533)\n",
      "760 ('11648-id_3', -9.567255407572533)\n",
      "761 ('11648-id_4', -9.567255407572533)\n",
      "762 ('11648-id_10', -9.567255407572533)\n",
      "763 ('11648-id_18', -9.567255407572533)\n",
      "764 ('11648-id_19', -9.567255407572533)\n",
      "765 ('11648-id_1', -9.567255407572533)\n",
      "766 ('11648-id_15', -9.567255407572533)\n",
      "767 ('16948-art05', -9.567255407572533)\n",
      "768 ('16948-art13', -9.567255407572533)\n",
      "769 ('16948-art06', -9.567255407572533)\n",
      "770 ('16948-art04', -9.567255407572533)\n",
      "771 ('16948-art03', -9.567255407572533)\n",
      "772 ('16948-art14', -9.567255407572533)\n",
      "773 ('16948-art09', -9.567255407572533)\n",
      "774 ('16948-art19', -9.567255407572533)\n",
      "775 ('16948-art17', -9.567255407572533)\n",
      "776 ('16948-art20', -9.567255407572533)\n",
      "777 ('16948-art21', -9.567255407572533)\n",
      "778 ('16948-art01', -9.567255407572533)\n",
      "779 ('16948-art18', -9.567255407572533)\n",
      "780 ('16948-art02', -9.567255407572533)\n",
      "781 ('16948-art10', -9.567255407572533)\n",
      "782 ('16948-art12', -9.567255407572533)\n",
      "783 ('16948-art11', -9.567255407572533)\n",
      "784 ('16948-art08', -9.567255407572533)\n",
      "785 ('16948-art07', -9.567255407572533)\n",
      "786 ('16948-art16', -9.567255407572533)\n",
      "787 ('16948-art00', -9.567255407572533)\n",
      "788 ('15889-art16', -9.567255407572533)\n",
      "789 ('15889-art06', -9.567255407572533)\n",
      "790 ('15889-art05', -9.567255407572533)\n",
      "791 ('15889-art04', -9.567255407572533)\n",
      "792 ('15889-art13', -9.567255407572533)\n",
      "793 ('15889-art20', -9.567255407572533)\n",
      "794 ('15889-art19', -9.567255407572533)\n",
      "795 ('15889-art24', -9.567255407572533)\n",
      "796 ('15889-art25', -9.567255407572533)\n",
      "797 ('15889-art23', -9.567255407572533)\n",
      "798 ('15889-art26', -9.567255407572533)\n",
      "799 ('15889-art22', -9.567255407572533)\n",
      "800 ('15889-art21', -9.567255407572533)\n",
      "801 ('15889-art10', -9.567255407572533)\n",
      "802 ('15889-art12', -9.567255407572533)\n",
      "803 ('15889-art11', -9.567255407572533)\n",
      "804 ('15889-art09', -9.567255407572533)\n",
      "805 ('15889-art07', -9.567255407572533)\n",
      "806 ('15889-art08', -9.567255407572533)\n",
      "807 ('15889-art15', -9.567255407572533)\n",
      "808 ('15889-art14', -9.567255407572533)\n",
      "809 ('24322-art03', -9.567255407572533)\n",
      "810 ('24322-art04', -9.567255407572533)\n",
      "811 ('24322-art19', -9.567255407572533)\n",
      "812 ('24322-art18', -9.567255407572533)\n",
      "813 ('24322-art16', -9.567255407572533)\n",
      "814 ('24322-art15', -9.567255407572533)\n",
      "815 ('24322-art21', -9.567255407572533)\n",
      "816 ('24322-art22', -9.567255407572533)\n",
      "817 ('24322-art14', -9.567255407572533)\n",
      "818 ('24322-art23', -9.567255407572533)\n",
      "819 ('24322-art02', -9.567255407572533)\n",
      "820 ('24322-art01', -9.567255407572533)\n",
      "821 ('24322-art08', -9.567255407572533)\n",
      "822 ('24322-art10', -9.567255407572533)\n",
      "823 ('24322-art09', -9.567255407572533)\n",
      "824 ('24322-art13', -9.567255407572533)\n",
      "825 ('24322-art12', -9.567255407572533)\n",
      "826 ('24322-art20', -9.567255407572533)\n",
      "827 ('24322-art06', -9.567255407572533)\n",
      "828 ('24322-art05', -9.567255407572533)\n",
      "829 ('24322-art07', -9.567255407572533)\n",
      "830 ('24322-art17', -9.567255407572533)\n",
      "831 ('15708-ref5', -9.567255407572533)\n",
      "832 ('15708-ref4', -9.567255407572533)\n",
      "833 ('15708-ref3', -9.567255407572533)\n",
      "834 ('15708-ref6', -9.567255407572533)\n",
      "835 ('15708-ref13', -9.567255407572533)\n",
      "836 ('15708-ref12', -9.567255407572533)\n",
      "837 ('15708-ref19', -9.567255407572533)\n",
      "838 ('15708-ref11', -9.567255407572533)\n",
      "839 ('15708-ref17', -9.567255407572533)\n",
      "840 ('15708-ref15', -9.567255407572533)\n",
      "841 ('15708-ref18', -9.567255407572533)\n",
      "842 ('15708-ref14', -9.567255407572533)\n",
      "843 ('15708-ref7', -9.567255407572533)\n",
      "844 ('15708-ref8', -9.567255407572533)\n",
      "845 ('15708-ref2', -9.567255407572533)\n",
      "846 ('15708-ref1', -9.567255407572533)\n",
      "847 ('15708-ref9', -9.567255407572533)\n",
      "848 ('15708-ref10', -9.567255407572533)\n",
      "849 ('13443-misc2', -9.567255407572533)\n",
      "850 ('13443-misc1', -9.567255407572533)\n",
      "851 ('13443-med1', -9.567255407572533)\n",
      "852 ('13443-bot1', -9.567255407572533)\n",
      "853 ('13443-civ3', -9.567255407572533)\n",
      "854 ('13443-mech1', -9.567255407572533)\n",
      "855 ('13443-civ2', -9.567255407572533)\n",
      "856 ('13443-elec1', -9.567255407572533)\n",
      "857 ('13443-elec3', -9.567255407572533)\n",
      "858 ('13443-elec2', -9.567255407572533)\n",
      "859 ('13443-tech1', -9.567255407572533)\n",
      "860 ('11649-id_12', -9.567255407572533)\n",
      "861 ('11649-id_13', -9.567255407572533)\n",
      "862 ('11649-id_10', -9.567255407572533)\n",
      "863 ('11649-id_9', -9.567255407572533)\n",
      "864 ('11649-id_8', -9.567255407572533)\n",
      "865 ('11649-id_7', -9.567255407572533)\n",
      "866 ('11649-id_4', -9.567255407572533)\n",
      "867 ('11649-id_14', -9.567255407572533)\n",
      "868 ('11649-id_1', -9.567255407572533)\n",
      "869 ('11649-id_11', -9.567255407572533)\n",
      "870 ('11649-id_3', -9.567255407572533)\n",
      "871 ('11649-id_5', -9.567255407572533)\n",
      "872 ('11649-id_6', -9.567255407572533)\n",
      "873 ('15050-IX_1', -9.567255407572533)\n",
      "874 ('15050-III_4', -9.567255407572533)\n",
      "875 ('15050-VI_2', -9.567255407572533)\n",
      "876 ('15050-XI_1', -9.567255407572533)\n",
      "877 ('15050-III_2', -9.567255407572533)\n",
      "878 ('15050-III_3', -9.567255407572533)\n",
      "879 ('15050-III_1', -9.567255407572533)\n",
      "880 ('15050-VI_1', -9.567255407572533)\n",
      "881 ('15050-XII_2', -9.567255407572533)\n",
      "882 ('15050-XII_1', -9.567255407572533)\n",
      "883 ('15050-VIII_1', -9.567255407572533)\n",
      "884 ('15050-IV_4', -9.567255407572533)\n",
      "885 ('15050-IV_5', -9.567255407572533)\n",
      "886 ('15050-IV_3', -9.567255407572533)\n",
      "887 ('15050-IV_1', -9.567255407572533)\n",
      "888 ('15050-IV_2', -9.567255407572533)\n",
      "889 ('15050-VII_1', -9.567255407572533)\n",
      "890 ('15050-X_2', -9.567255407572533)\n",
      "891 ('15050-I_1', -9.567255407572533)\n",
      "892 ('15050-V_1', -9.567255407572533)\n",
      "893 ('15050-X_1', -9.567255407572533)\n",
      "894 ('15050-II_1', -9.567255407572533)\n",
      "895 ('14990-art02', -9.567255407572533)\n",
      "896 ('14990-art13', -9.567255407572533)\n",
      "897 ('14990-art05', -9.567255407572533)\n",
      "898 ('14990-art16', -9.567255407572533)\n",
      "899 ('14990-art15', -9.567255407572533)\n",
      "900 ('14990-art08', -9.567255407572533)\n",
      "901 ('14990-art03', -9.567255407572533)\n",
      "902 ('14990-art10', -9.567255407572533)\n",
      "903 ('14990-art11', -9.567255407572533)\n",
      "904 ('14990-art07', -9.567255407572533)\n",
      "905 ('14990-art06', -9.567255407572533)\n",
      "906 ('14990-art04', -9.567255407572533)\n",
      "907 ('14990-art01', -9.567255407572533)\n",
      "908 ('14990-art09', -9.567255407572533)\n",
      "909 ('14990-art12', -9.567255407572533)\n",
      "910 ('14990-art14', -9.567255407572533)\n",
      "911 ('15193-art21', -9.567255407572533)\n",
      "912 ('15193-art17', -9.567255407572533)\n",
      "913 ('15193-art18', -9.567255407572533)\n",
      "914 ('15193-art05', -9.567255407572533)\n",
      "915 ('15193-art06', -9.567255407572533)\n",
      "916 ('15193-art04', -9.567255407572533)\n",
      "917 ('15193-art03', -9.567255407572533)\n",
      "918 ('15193-art15', -9.567255407572533)\n",
      "919 ('15193-art27', -9.567255407572533)\n",
      "920 ('15193-art13', -9.567255407572533)\n",
      "921 ('15193-art23', -9.567255407572533)\n",
      "922 ('15193-art22', -9.567255407572533)\n",
      "923 ('15193-art19', -9.567255407572533)\n",
      "924 ('15193-art29', -9.567255407572533)\n",
      "925 ('15193-art28', -9.567255407572533)\n",
      "926 ('15193-art26', -9.567255407572533)\n",
      "927 ('15193-art01', -9.567255407572533)\n",
      "928 ('15193-art25', -9.567255407572533)\n",
      "929 ('15193-art11', -9.567255407572533)\n",
      "930 ('15193-art08', -9.567255407572533)\n",
      "931 ('15193-art20', -9.567255407572533)\n",
      "932 ('15193-art24', -9.567255407572533)\n",
      "933 ('15193-art07', -9.567255407572533)\n",
      "934 ('15193-art12', -9.567255407572533)\n",
      "935 ('15193-art09', -9.567255407572533)\n",
      "936 ('15193-art10', -9.567255407572533)\n",
      "937 ('24323-art05', -9.567255407572533)\n",
      "938 ('24323-art20', -9.567255407572533)\n",
      "939 ('24323-art12', -9.567255407572533)\n",
      "940 ('24323-art02', -9.567255407572533)\n",
      "941 ('24323-art18', -9.567255407572533)\n",
      "942 ('24323-art19', -9.567255407572533)\n",
      "943 ('24323-art01', -9.567255407572533)\n",
      "944 ('24323-art25', -9.567255407572533)\n",
      "945 ('24323-art07', -9.567255407572533)\n",
      "946 ('24323-art24', -9.567255407572533)\n",
      "947 ('24323-art11', -9.567255407572533)\n",
      "948 ('24323-art04', -9.567255407572533)\n",
      "949 ('24323-art26', -9.567255407572533)\n",
      "950 ('24323-art03', -9.567255407572533)\n",
      "951 ('24323-art10', -9.567255407572533)\n",
      "952 ('24323-art21', -9.567255407572533)\n",
      "953 ('24323-art09', -9.567255407572533)\n",
      "954 ('24323-art22', -9.567255407572533)\n",
      "955 ('24323-art17', -9.567255407572533)\n",
      "956 ('24323-art13', -9.567255407572533)\n",
      "957 ('24323-art14', -9.567255407572533)\n",
      "958 ('24323-art15', -9.567255407572533)\n",
      "959 ('24323-art16', -9.567255407572533)\n",
      "960 ('24323-art08', -9.567255407572533)\n",
      "961 ('24323-art23', -9.567255407572533)\n",
      "962 ('18265-art03', -9.567255407572533)\n",
      "963 ('18265-art01', -9.567255407572533)\n",
      "964 ('18265-art04', -9.567255407572533)\n",
      "965 ('18265-art08', -9.567255407572533)\n",
      "966 ('18265-art14', -9.567255407572533)\n",
      "967 ('18265-art16', -9.567255407572533)\n",
      "968 ('18265-art10', -9.567255407572533)\n",
      "969 ('18265-art11', -9.567255407572533)\n",
      "970 ('18265-art13', -9.567255407572533)\n",
      "971 ('18265-art06', -9.567255407572533)\n",
      "972 ('18265-art05', -9.567255407572533)\n",
      "973 ('18265-art17', -9.567255407572533)\n",
      "974 ('18265-art07', -9.567255407572533)\n",
      "975 ('8952-id_69', -9.660878039105755)\n"
     ]
    }
   ],
   "source": [
    "def get_all_doc_ids(inverted_index):\n",
    "    doc_ids = set()\n",
    "    for posting in inverted_index.values():\n",
    "        for doc_id in posting.doc_to_term.keys():\n",
    "            doc_ids.add(doc_id)\n",
    "    \n",
    "    return list(doc_ids)\n",
    "\n",
    "def get_num_words_by_doc(inverted_index, doc_id):\n",
    "    for posting in inverted_index.values():\n",
    "        if doc_id in posting.doc_to_term.keys():\n",
    "            return posting.doc_to_term[doc_id][2]\n",
    "    return 0\n",
    "\n",
    "def get_story_id(inverted_index, doc_id):\n",
    "    for posting in inverted_index.values():\n",
    "        if doc_id in posting.doc_to_id.keys():\n",
    "            return posting.doc_to_id[doc_id]\n",
    "    return \"\"\n",
    "\n",
    "def query_likelihood(inverted_index, wordPhrases):\n",
    "    \"\"\"\n",
    "      Evaluate a QL query over all wordPhrases\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          wordPhrases: a list of words or phrases to search for.\n",
    "\n",
    "      Returns: a set of tuples, where the first value is the doc_id, and the second is the score\n",
    "    \"\"\"\n",
    "    # need to keep track of number of words per document, maybe edit data structure\n",
    "    # c_qi frequency of term qi in the collection (.total_term_count)\n",
    "    # |D| = number of words in document (doc_to_term[2])\n",
    "    # f_qi,D = number of times word q_i occurs in document (doc_id: term_count)\n",
    "    # |C| = total number of words in the collection \n",
    "    doc_id_lst = get_all_doc_ids(inverted_index)\n",
    "    ranking = []\n",
    "    mu = 300  # Dirichlet smoothing parameter\n",
    "    phrase_lst = [phrase for phrase in wordPhrases.split(\"\\t\") if phrase] # list of phrases in the query [[phrase1], [phrase2]]\n",
    "    C = get_total_word_count(inverted_index)\n",
    "    \n",
    "    for doc_id in doc_id_lst:\n",
    "        pqd = 0\n",
    "\n",
    "        for phrase in phrase_lst:\n",
    "            word_lst = phrase.split()\n",
    "            n = len(word_lst)\n",
    "            if n == 1 and all(True if word in inverted_index.keys() else False for word in word_lst):\n",
    "                c_qi = inverted_index[word_lst[0]].total_term_count\n",
    "                f_qi = inverted_index[word_lst[0]].doc_to_term[doc_id][0] if doc_id in inverted_index[word_lst[0]].doc_to_term.keys() else 0\n",
    "                D = inverted_index[word_lst[0]].doc_to_term[doc_id][2] if doc_id in inverted_index[word_lst[0]].doc_to_term.keys() else 0\n",
    "                pqd += math.log( (f_qi + mu*(c_qi/C))/(D + mu) )\n",
    "\n",
    "            elif n > 1 and all(True if word in inverted_index.keys() else False for word in word_lst):\n",
    "                c_qi = collection_frequency(inverted_index, phrase)\n",
    "                f_qi = tf(inverted_index, doc_id, phrase)\n",
    "                D = get_num_words_by_doc(inverted_index, doc_id)\n",
    "                pqd += math.log( (f_qi + mu*(c_qi/C)) / (D + mu) )\n",
    "                \n",
    "        story_id = get_story_id(inverted_index, doc_id)\n",
    "        ranking.append((story_id, pqd))\n",
    "    \n",
    "    return ranking\n",
    "# test_phrase = \"improvement in\\timprovement\"\n",
    "# test_phrase = \"united states\\t\"\n",
    "test_phrase = \"hospital\\t\"\n",
    "t = query_likelihood(inverted_index, test_phrase)\n",
    "for i, r in enumerate(sorted(t, key=lambda x: x[1], reverse=True)):\n",
    "    print(i, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "WHf2GC3EOMwo"
   },
   "outputs": [],
   "source": [
    "def bm25(inverted_index, words):\n",
    "    \"\"\"\n",
    "      Perform a bm25 query over all words in the query\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          words: a list of words or phrases to search for.\n",
    "\n",
    "      Returns: an array of tuples, where the first value is the doc_id, and the second is the score\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    k1 = 1.8\n",
    "    k2 = 5.0\n",
    "    b = 0.75\n",
    "\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6prrF_LGGLP"
   },
   "source": [
    "## 3. Implement `run_queries()`\n",
    "\n",
    "This is the only method that you will be graded on, which should create your inverted index based on the `document_fpath`, parse and evaluate all queries in `query_fpath` (name of queries file like *P3_eval.tsv*), and print out the results to a file named by `trecrun_file` (for example, *output.trecrun*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true,
    "id": "ldaX0-z3HvMC"
   },
   "outputs": [],
   "source": [
    "def run_queries(document_fpath, query_fpath, trecrun_file):\n",
    "    \"\"\"\n",
    "      Create an inverted index, parse a query file, run them on the inverted index,\n",
    "      and print out the results in trecrun format.\n",
    "\n",
    "      Args:\n",
    "          document_fpath: the name of a file containing the documents to index\n",
    "          query_fpath: the name of the file containing queries to run\n",
    "          trecrun_file: the name of the file that will be created to contain the\n",
    "                        output of running the queries\n",
    "\n",
    "      Returns: void\n",
    "    \"\"\"\n",
    "\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "# Example usage (generates output.trecrun to compare to P3train.trecrun)\n",
    "run_queries(\"sciam.json.gz\", \"P3eval.tsv\", \"output.trecrun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg17qtuS8_l7"
   },
   "source": [
    "### 3.1 Sample queries and answers\n",
    "\n",
    "We are providing *P3train.tsv* that you can use to try out your system. We are also providing you with the expected output for those queries as *P3train.trecrun*. Your code will be tested on those same queries plus a number of other queries, so please try other possibilities: do not assume that because your code handles the training queries your code is perfect. You may use the below method, *print_file()*, to print out *P3train.trecrun* to compare your results manually. You might want to create a variation that also reads in your output and does a comparison to find where things are different. However, this part will not be graded, and is optional to implement. As always, just make sure that it does not trigger an error in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true,
    "id": "XrgAN9L1PM4_"
   },
   "outputs": [],
   "source": [
    "def print_file(file_path):\n",
    "    \"\"\"\n",
    "      Print out the contents of a file\n",
    "\n",
    "      Args:\n",
    "          file_path: the path to the file\n",
    "\n",
    "      Returns: n/a\n",
    "    \"\"\"\n",
    "\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "# Example usage\n",
    "print_file(\"P3train.trecrun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "EujWLMszLBmd"
   },
   "outputs": [],
   "source": [
    "def print_queryfile(file_path):\n",
    "    \"\"\"\n",
    "      Print out the contents of a file\n",
    "\n",
    "      Args:\n",
    "          file_path: the path to the file\n",
    "\n",
    "      Returns: n/a\n",
    "    \"\"\"\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "# Example usage\n",
    "print_queryfile(\"P3train.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2i8QsRrUdUop"
   },
   "source": [
    "## 4. Analysis Questions\n",
    "\n",
    "Answer the questions below. You will probably find it easiest to answer some of them if you engineer one of your functions above to display these numbers after indexing and then you can copy them below.\n",
    "\n",
    "Unlike the way P1 and P2 were implemented, these analysis questions are designed to be handled by the autograder, though they will be hidden \"tests\" so you will not have confirmation that you got them right until after grading is completed. If the answer is a float, then reduce to 2 decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX7JayJmdYdr"
   },
   "source": [
    "**4.1**. What is the average length of a story in the *sciam* collection? What is the shortest story (and how short it is)? What is the longest story (and how long is it)? Note that for this project, \"short\" and \"long\" are measured by the number of tokens, not the number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "error",
     "timestamp": 1733687967283,
     "user": {
      "displayName": "Huy Tran",
      "userId": "16988764098922800274"
     },
     "user_tz": 300
    },
    "id": "qTwBJjsbf_Ri",
    "outputId": "0324a338-b686-4a0f-be08-b23b5c8de278"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3845089107.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[272], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    averageStoryLength =\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Provide the correct values for each of these \"variables\" as described above. Do not provide any other code.\n",
    "\n",
    "def analysis_q1():\n",
    "  averageStoryLength =\n",
    "\n",
    "  shortestStoryStoryId = \"\"\n",
    "  shortestStoryLength =\n",
    "\n",
    "  longestStoryStoryId = \"\"\n",
    "  longestStoryLength =\n",
    "\n",
    "  return [averageStoryLength, shortestStoryStoryId, shortestStoryLength, longestStoryStoryId, longestStoryLength]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5ShTxDede52"
   },
   "source": [
    "**4.2**. What word occurs in the most stories and how many stories does it occur in? What word has the largest number of occurrences and how many does it have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "error",
     "timestamp": 1733687967284,
     "user": {
      "displayName": "Huy Tran",
      "userId": "16988764098922800274"
     },
     "user_tz": 300
    },
    "id": "z4ejE0afgtJZ",
    "outputId": "e03c9394-2283-45d9-d550-0b0e2bbef72a"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-f57dd5f873c0>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-f57dd5f873c0>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    numberOfStoriesItOccursIn =\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Provide the correct values for each of these \"variables\" as described above. Do not provide any other code.\n",
    "\n",
    "def analysis_q2():\n",
    "\n",
    "  termOccurringInMostStories = \"\"\n",
    "  numberOfStoriesItOccursIn =\n",
    "  termOccurringMostFrequently = \"\"\n",
    "  numberOfTimesItOccurs =\n",
    "\n",
    "  return [termOccurringInMostStories, numberOfStoriesItOccursIn, termOccurringMostFrequently, numberOfTimesItOccurs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzeT9jwfdhHI"
   },
   "source": [
    "**4.3.** How many unique words are there in this collection? How many of them occur only once? What percent is that? Is that what you would expect? We are not asking you to return this, but to increase the chance that you have the right answer, you might think about why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1733687967284,
     "user": {
      "displayName": "Huy Tran",
      "userId": "16988764098922800274"
     },
     "user_tz": 300
    },
    "id": "eWuHE51Eg6lF"
   },
   "outputs": [],
   "source": [
    "# Provide the correct values for each of these \"variables\" as described above. Do not provide any other code.\n",
    "\n",
    "def analysis_q3():\n",
    "\n",
    "  numberUniqueWords =\n",
    "  numberWordsOccurringOnce =\n",
    "  percentOfWordsThatOccurOnce = # be sure this is a percent (without the %) and not a fraction\n",
    "\n",
    "  return [numberUniqueWords, numberWordsOccurringOnce, percentOfWordsThatOccurOnce]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "javDin3edkVO"
   },
   "source": [
    "**4.4**. Your training queries have two queries that are roughly about the scientific american supplement. Suppose that you wanted to judge stories for relevance using a pooling strategy that takes the top 100 documents from each of those two queries. How many unique documents will you be judging? What if you only considered the top 20? Suppose you had a budget that allowed you to judge at most 30 documents. How deeply could you go into the two queries for judging to get 30 judged, no more, no less?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "error",
     "timestamp": 1733687967284,
     "user": {
      "displayName": "Huy Tran",
      "userId": "16988764098922800274"
     },
     "user_tz": 300
    },
    "id": "MZ-UINLgQEhg",
    "outputId": "e31cdd05-75ca-49f7-8d35-29d984a780dc"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-1f50e9097b04>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-1f50e9097b04>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    numberUniqueJudged100 =\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Provide the correct values for each of these \"variables\" as described above. Do not provide any other code.\n",
    "\n",
    "def analysis_q4():\n",
    "\n",
    "  numberUniqueJudged100 =\n",
    "  numberUniqueJudged20 =\n",
    "  poolDepthFor30 =\n",
    "\n",
    "  return [numberUniqueJudged100, numberUniqueJudged20, poolDepthFor30]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2i8QsRrUdUop"
   ],
   "provenance": [
    {
     "file_id": "1R6zVQQrRXocEl34pf6RIGC_fKTtyWcN8",
     "timestamp": 1733687296905
    },
    {
     "file_id": "1lcHOxkKqqwEvqW0Aaj3GfHdvAjqVkmFu",
     "timestamp": 1732342610684
    },
    {
     "file_id": "1D2Q5vw_CPRBP8T-rBzulnRmzNMSpIr6m",
     "timestamp": 1732035932732
    },
    {
     "file_id": "14KZeo6W3-YPtuVCLe5qZVmmz3pY8bQeW",
     "timestamp": 1730955104744
    },
    {
     "file_id": "1PNLu5Ozbws477bBDEbW9ctbWjtDBFyeb",
     "timestamp": 1727172972777
    },
    {
     "file_id": "1V8FI-GcatOgRkuGy51OFy3zF1U4wti5r",
     "timestamp": 1727167265392
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
