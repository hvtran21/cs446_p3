{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq1WZzCeLqpK"
   },
   "source": [
    "Your name: Huy Tran\n",
    "\n",
    "Your student ID number: 33259590\n",
    "\n",
    "Shared link to this notebook: https://github.com/hvtran21/cs446_p3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhlgD8H1Ll5U"
   },
   "source": [
    "# Programming Assignment 3 (P3) for COMPSCI 446, Search Engines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9_kEMGent_J"
   },
   "source": [
    "This project is focused on indexing and query processing using a small collection of documents. Before starting, be sure to review Chapter 5 in the textbook, especially section 5.3, which covers inverted indexes, and section 5.6 (with an emphasis on 5.6.1), which discusses index construction. For the query processing part, sections 5.7.1 and 5.7.2, as well as sections 7.1.1, 7.2.2, and 7.3.1, will be particularly useful.\n",
    "\n",
    "Your task is to implement `index_run(file, query_list, trecrun_file)` method. This method will accomplish 3 things:\n",
    "* [Document Parsing](#document-parsing) : Parse the collection of documents in\n",
    "* [Query Processing](#query-processing) : Read and process a list of queries from `query_list`.\n",
    "* [TREC Run Formatting](#output-formatting) : Output the results in TREC run format (as in P2) to `trecrun_file`.\n",
    "\n",
    "Only the implementation of `index_run(file, query_list, trecrun_file)` will be graded. However, we will provide a skeleton of helper methods you can implement for this project. We will not test the provided helper functions or even verify that they exist. (Though please be certain that those code cells run, even if you do that by making an empty function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwNGCWtxoIXd"
   },
   "source": [
    "Here are a list of provided files that will be loaded into your Google Drive for use in this notebook:\n",
    "\n",
    "* _sciam.json.gz_, a compressed document collection comprising of a number of stories from issues of Scientific American from the 1800s. They are all from Project Gutenberg’s collection. The “books” were automatically broken into stories and each story is a “document” for purposes of this project. A URL was also automatically included that takes you to a nice version of the story if you’re interested in looking at it (or its images). These stories have been partially vetted for objectionable material (sensibilities in the 1800s were different than they are today); if you find something that we overlooked, please let us know.\n",
    "\n",
    "* _P3train.tsv_, a file containing a list of sample queries to run as input to your program for testing.\n",
    "\n",
    "* _P3train.trecrun_ is a file containing a sample, trecrun-formatted output for _P3train.tsv_.\n",
    "\n",
    "* _queries.tsv_, a file containing a list of queries that you will be graded on for your submission; we are not providing sample output for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwtOQuDMHIOy"
   },
   "source": [
    "##0. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hWAYQYm7AiFQ"
   },
   "outputs": [],
   "source": [
    "version = 2 # DO NOT MODIFY. If notebook does not match with autograder version, many tests are likely to fail!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-rdywfVn8Zd"
   },
   "source": [
    "###Link your Google Drive\n",
    "Execute the following to connect to Google Drive (you will be prompted repeatedly for access to your Google Drive; please give it permission) and download copies of the sample files listed above. You should not need to make any modifications to the code, though if you want to use a slightly different path in Google Drive, you can modify the appropriate drive_path value. (The autograder will not use your Google Drive and will fail if you insert code that explicitly uses the Google Drive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RQVcpdzFG08",
    "outputId": "34ed38f3-2c41-4b17-c0c1-cc31c111aeae"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import gzip\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    in_colab = True\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "\n",
    "\n",
    "# You are more than welcome to code some helper functions.\n",
    "# But do note that we are only grading functions that are coded in the template files.\n",
    "\n",
    "\n",
    "# Connect to Google Drive and download copies of the sample files listed above.\n",
    "# Please allow the access to your Google Drive or the following dataset loader will fail.\n",
    "# (The autograder will not use your Google Drive.)\n",
    "if in_colab:\n",
    "  drive.mount(\"/content/drive/\") ## DO NOT MODIFY THIS LINE\n",
    "  data_path = \"/content/drive/MyDrive/CS 446 Search Engines/p3\" ## CHANGE TO YOUR OWN FOLDER ON GOOGLE DRIVE\n",
    "else:\n",
    "  data_path = \"./data/\"  ## DO NOT MODIFY THIS LINE. CHANGING THIS LINE WOULD RESULT IN FAIL OF AUTOGRADER TESTS\n",
    "\n",
    "assert os.path.exists(data_path), \"Change data_path to a valid and existing file path in your google drive!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWKbjCm2FiXZ"
   },
   "source": [
    "### Load the provided files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDnungmrkPsj",
    "outputId": "defeece6-6fcb-4aac-a908-acef3b3ffb47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"sciam.json.gz\" already exists, not downloading.\n",
      "File \"P3train.trecrun\" already exists, not downloading.\n",
      "File \"P3train.tsv\" already exists, not downloading.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import gzip\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_file(file_path: str, gz_zip: bool = True, is_json: bool=False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load strings from the text or gzip file. Remember to strip newline if necessary!\n",
    "\n",
    "    Args:\n",
    "        file_path: the location of file we want to analyze on.\n",
    "        gz_zip: true if the file is gzipped, false otherwise\n",
    "\n",
    "    Returns: an array of string loaded from the text or gzip file.\n",
    "    \"\"\"\n",
    "    webloc = \"https://cs.umass.edu/~allan/cs446/\"\n",
    "\n",
    "\n",
    "    data_info = Path(data_path)\n",
    "    if not data_info.exists() or not data_info.is_dir():\n",
    "      print(f\"Google folder \\\"{data_path}\\\" is not present or not a folder. Nothing will work from here.\")\n",
    "      return []\n",
    "\n",
    "    local_google_drive_path = os.path.join(data_path,file_path)\n",
    "    local_file = Path(local_google_drive_path)\n",
    "    if local_file.is_file():\n",
    "        print(f\"File \\\"{file_path}\\\" already exists, not downloading.\")\n",
    "    else:\n",
    "        print(f\"Cannot find \\\"{file_path}\\\" so downloading it\")\n",
    "        urllib.request.urlretrieve(webloc + file_path, local_google_drive_path)\n",
    "        print(\"Done\")\n",
    "\n",
    "    results = []\n",
    "    if not gz_zip:\n",
    "        f = open(local_google_drive_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    else:\n",
    "        # Read compressed file (opened in text mode since that's what we use)\n",
    "        f = gzip.open(local_google_drive_path, \"rt\", encoding=\"utf-8\")\n",
    "\n",
    "    # Pull in the contents of the file and return it\n",
    "    if is_json:\n",
    "      results = json.load(f)['corpus']\n",
    "    else:\n",
    "      results = [line.strip(\"\\n\") for line in f.readlines() if line]\n",
    "\n",
    "    if f:\n",
    "      f.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# path to documents and stopwords\n",
    "documents_path = \"sciam.json.gz\" # path to gzip file that contains documents\n",
    "P3_train_path = \"P3train.tsv\" # Path to p1 train dataset\n",
    "P3_trecrun_path = \"P3train.trecrun\"  # path to the file that contains stopwords\n",
    "\n",
    "documents = load_file(documents_path, is_json=True)\n",
    "trecrun  = load_file(P3_trecrun_path, gz_zip = False)    # Downloading sample queries file to folder, these are \\t seperated files (Not in json format)\n",
    "queries  = load_file(P3_train_path, gz_zip = False)      # Download sample queries output file to folder, these are \\t seperated files (Not in json format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaMl5vRzIp8i"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset is stories from Scientific American issues from the 1800s. The data has been tokenized and stemmed (not stopped) and made available as sciam.json.gz. (You will probably want to look at it uncompressed, but be sure that your code handles the compressed version only.) The stories were extracted from 25 issues of the magazine. If you're curious, you can look at the original magazine issues at https://www.gutenberg.org/ebooks/bookshelf/256.\n",
    "\n",
    "This dataset has been preprocessed by stripping out punctuation and applying the Krovetz Stemmer (not Porter). No stopwords have been removed. Note that this means that the tokenization and stemming steps (as in P1) have been performed and, because stemming has already happened, that _you will not use a stopword list_. (Plus, of course, this will allow you to support queries with conjunction words -- e.g., the three words “clumps and keys” in order!)\n",
    "\n",
    "An example story is below. The text element is a actually single line in the input file but is presented wrapped here for ease of presentation.  The tokens are always separated by space characters, meaning that you will essentially be using a space tokenizer!\n",
    "\n",
    "```\n",
    "{  \"article\" : \"8952\",\n",
    "   \"storyID\" : \"8952-id_18\",\n",
    "   \"storynum\" : \"id_18\",\n",
    "   \"url\" : \"https://www.gutenberg.org/cache/epub/8952/pg8952-images.html#id_18\",\n",
    "   \"text\" : \"invention patent in england by americacompile from the journal of\n",
    "   the commissioner of patentprovisional protection for six month3 201 sewing\n",
    "   machine h a house bridgeport conn november 4 1869 3 211 bore tool alexander\n",
    "   allen new york city november 5 1869 3 215 mode of and device foe secure\n",
    "   stair rod h uhry new york city november 6 1869 3 229 transportation of\n",
    "   letters parcel and other freight by atmospheric pressure and in apparatus\n",
    "   connected therewith a e beach stratford conn november 9 1869 3 303 reload\n",
    "   cartridge shell r j gatl indianapolis ind november 16 1869 3 342 wooden\n",
    "   pavement i hayward and j f paul boston mass november 20 1869 3 358 machinery\n",
    "   for distribute type o l brown boston mass november 20 1869 3 219 weigh\n",
    "   machine m kennedy new york city november 10 1869 3 260 bran duster w huntley\n",
    "   and a babcock silver creek n y november 12 1869 3 339 railway carriage e\n",
    "   robbin cincinnati ohio november 19 1869 3 341 revolving battery gun r j gatl\n",
    "   indianapolis ind nov 19 1869 3 360 sash fastener s l loomi south byron n y\n",
    "   november 20 1869 3 363 magnetic machine and magnet j burrough jr newark n j\n",
    "   november 20 1869  \"  \n",
    "}\n",
    "```\n",
    "\n",
    "This story is from article number 8952 published in Scientific American, Volume 22, No. 1, January 1, 1870. The story has an internal number of `id_18`: the storyID combines the article number and the story number to yield `8952-id_18`. The URL will get you to that story directly. If you scroll to the top you'll see the article's date.\n",
    "\n",
    "For purposes of P3, only the storyID and the text are important. You may find the others helpful, particularly the URL if you want to decipher some of these documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucmDVvJuFDU1"
   },
   "source": [
    "<a name=\"document-parsing\"></a>\n",
    "## 1. Parse the collection of documents in `file` to build an in-memory index.\n",
    "\n",
    "To implement the first component of `index_run()`, we want to read in the tokenized and stemmed document collection provided and build an inverted index. Here are some things you will probably want to take into account.\n",
    "\n",
    "- The documents are stored using JSON, so you'll need to read them in that way.\n",
    "Python has a JSON library in the standard library which should suffice; we discourage you from spending the time to parse JSON on your own!\n",
    "- Because of the pre-processing we have done for you, term vectors should be constructed by splitting the text field's value by spaces (the regex \\\\s+), and ignoring blank strings. You've already shown you can tokenize, stop, and stem, so we're making it easier for this project.\n",
    " - Note that the processing in this case did not squeeze apostrophes out: instead, they were treated as word separators. That means that the word `quaker's` has resulted in tokens `quaker` and also `s`. Similarly, you'll find `t` (from `don't`) and `ve` (from `I've`) and so on as tokens. You will handle them as if they were normal words: do not remove them! That way the phrase query `\"quaker s\"` (see Part II) should find the word `quaker's` (and also `quaker s'truth` if by some weird chance that appeared in the stories).\n",
    "- Note that the index includes Unicode characters, but you should not worry about them even if they create odd tokens. Tokens are whatever is separated by a whitespace.\n",
    "- You need to build a simple inverted index with count and positional information: a combination of Figure 5.4 (p.134) and Figure 5.5 (p.135) in the textbook. (Depending on how you store things, the count may come along for free with the position information.) While you may store your index to disk to enable reuse, it is not required: an in-memory only index is sufficient for the project. (Your decision may be made partly by how long it takes to process the full collection in your implementation. It should be fast.)\n",
    "\n",
    "- Here are some recommendations regarding the data structures you will use behind these functions to implement the indea. You can use whatever data structures you feel are appropriate to implement these functcions, however, you may find these ideas helpful. We will not explicitly test your data structures.\n",
    "  - We recommend having the data storage behind your inverted index being of a form similar to **Dict[str, List[Posting]]**. Other data structures are plausible but are unlikely to provide any particular value for this project.\n",
    " - You might define a **PostingList** class that provides the **List[Posting]** datatype. Mostly that would help abstract away the implementation of the list.\n",
    " - **Posting** might be a **DocId** and **Positions** pair, so that a dictionary is used to find inverted lists and the inverted list is a list of \"Postings.\" It might also include the count of postings in that document.\n",
    " - **Positions** might be a **List[int]** or **array** of integers\n",
    " - **For the sake of efficiency (string comparisons are slow)** and memory usage, we encourage you to encode document numbers as integers, not using the *storyID* textual field. Otherwise you will not be able to answer analysis questions about compression. However, note that you will still need to be able to convert a document number back into its corresponding *storyID* for the TREC run output file. For example: getStoryID(1) would be “8951-id_1” since that's the first story in *sciam.json.gz*.\n",
    "\n",
    "- We suggest you think ahead (see [processing queries](#query-processing) below) and ensure your index can access the vocabulary, term counts, document counts and other statistics that you will need to perform the query evaluation activities. Thinking this through now will save some headaches later.\n",
    "\n",
    " - You will be writing code to handle arbitrary (but simple) queries for phrases and terms. That means your code will benefit from being general in how it handles them. You will likely find it best to have a method that allows any number of terms and/or phrases and does not depend on knowing what those terms and/or phrases will be or even how many there will be. Here are a couple of tips or ideas that may or may not help your thinking:\n",
    "   - You will be implementing scoring functions and models that produce scores that are not just integers. Put some time into it now (read ahead!) to ensure that your inverted index provides everything that is needed for those operations -- e.g., the number of terms, the number of documents, and so on. What do you think you might need to store and where would it make sense to store it?\n",
    "   - Would it benefit you to define helper functions for processes such as counting phrases and terms?\n",
    "   - Can you implement terms as phrases of length 1? That is, have one function that handles phrases and single terms that doesn't care whether there are 1 or 32 terms in the phrase. That provides a cleaner abstraction.\n",
    "\n",
    "Below is the stub of a helper function that can be implemented to build and return an inverted index for `index_run()`. You are not required to use this, and it will not be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "KTdbdwisLUB3",
    "outputId": "fd825cbd-c034-4075-9b41-862c7b30f1d0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "\n",
    "# postings can be doc id: term count, list[positions]\n",
    "# how inverted list should be implemented:\n",
    "#    term: Posting()\n",
    "#        -> where object posting can be defined as shown above\n",
    "# need to make it easy to get statistics, like term counts, document counts so on and so forth.\n",
    "# need to use json parsing library (python standard) to make parsing file easier\n",
    "# -> list of dictionaries i believe or something.\n",
    "\n",
    "\"\"\"\n",
    "a posting is mapping from a single term that gives information that:\n",
    "    -> total times it occurs in the entire corpus\n",
    "    -> a list of documents that the term occurs in\n",
    "        -> the positions of which the term occurs in that document\n",
    "        -> the number of time that term occurs in that document\n",
    "\n",
    "\"\"\"\n",
    "def load_json(file_path: str, is_json: bool) -> None:\n",
    "    f = gzip.open(file_path, \"rt\", encoding=\"utf-8\")\n",
    "\n",
    "    if is_json:\n",
    "        results = json.load(f)['corpus']\n",
    "    else:\n",
    "        results = [line.strip(\"\\n\") for line in f.readlines() if line]\n",
    "    \n",
    "    return results\n",
    "\n",
    "class Posting:\n",
    "    def __init__(self):\n",
    "        self.total_term_count = 0\n",
    "        self.doc_to_id = {}     # doc number: storyID\n",
    "        self.id_to_doc = {}     # storyID: doc number\n",
    "        self.doc_to_term = {}   # doc_id: [term_count, [positions]]   \n",
    "\n",
    "    def add_story_id(self, document_number: int, storyID: str) -> None:\n",
    "        self.doc_to_id[document_number] = storyID\n",
    "        self.id_to_doc[storyID] = document_number\n",
    "    \n",
    "    # doc_id will correspond to the document number\n",
    "    def add_doc_id_info(self, doc_id: int, term_count: int, positions: list[int]) -> None:\n",
    "        if doc_id in self.doc_to_term.keys(): # if we want to add a new term and the doc_id already exists, we just want to update the information\n",
    "            new_count = term_count + self.doc_to_term[doc_id][0] # first position is term count\n",
    "            self.doc_to_term[doc_id][1].extend(positions)\n",
    "            self.doc_to_term.update({doc_id: [new_count, self.doc_to_term[doc_id][1]]})\n",
    "\n",
    "        else:\n",
    "            self.doc_to_term[doc_id] = [term_count, positions]\n",
    "\n",
    "    def get_document_occurence(self): # returns the number of documents that this term occurs in\n",
    "        return len(self.doc_to_term.keys())\n",
    "        \n",
    "def build_inverted_index(file):\n",
    "    inverted_index = {}\n",
    "\n",
    "    for i, dictionary in enumerate(file):\n",
    "        document = dictionary.get('text')\n",
    "        positions = {}\n",
    "\n",
    "        for position, word in enumerate(document.split()): # obtains word: list(type: int, contains: zero-indexed positions)\n",
    "            if word:\n",
    "                positions.setdefault(word, []).append(position)\n",
    "            \n",
    "        for word, positions in positions.items():\n",
    "            if word not in inverted_index:\n",
    "                new_posting = Posting()\n",
    "                new_posting.add_story_id(i, dictionary.get(\"storyID\"))\n",
    "                new_posting.add_doc_id_info(i, len(positions), positions)\n",
    "                inverted_index[word] = new_posting\n",
    "            else:\n",
    "                inverted_index[word].add_story_id(i, dictionary.get(\"storyID\"))\n",
    "                inverted_index[word].add_doc_id_info(i, len(positions), positions)\n",
    "\n",
    "            inverted_index[word].total_term_count += len(positions)\n",
    "    return inverted_index\n",
    "\n",
    "# Example usage\n",
    "d = load_json(\"./data/sciam.json.gz\", is_json=True)\n",
    "inverted_index = build_inverted_index(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1fS3x8rJhsC"
   },
   "source": [
    "### 1.1 Debugging the Index\n",
    "\n",
    "We recommend that you consider a helper method to print out some information to help you debug your index. There are other ways to do this, but we're setting things up to ensure you can do it this way if you like. We will not test this function or even verify that it exists. (Though please make sure the cell does not trigger an error.)\n",
    "\n",
    "For example, you might print out some information that will be helpful for verifying your index overall for some array of terms. It could first print the number of documents, the number of unique terms that are in the collection, and the total number of term occurrences. Then, for each term listed, it could print out one line with the term, the number of documents containing that term, and the total number of occurrences of that term.\n",
    "\n",
    "Here is some sample output that you should get for this data using this collection, but since we will not actually test your code (it is for debugging only), the actual format does not matter.\n",
    "\n",
    "```\n",
    "debug_inverted_index(inverted_index, [\"and\",\"the\", \"elephant\",\"science\"], showTerms=False)\n",
    "\n",
    "976 docs, 27217 terms, 1185999 occurrences\n",
    "and 956 docs, 34896 occurrences\n",
    "the 966 docs, 96151 occurrences\n",
    "elephant 7 docs, 18 occurrences\n",
    "science 112 docs, 251 occurrences\n",
    "\n",
    "```\n",
    "\n",
    "You might also consider creating something that prints out parts of the inverted list so you can see what's there. For example, you might have the argument “showTerms” that prints out the inverted list for the listed terms. For example, you might show something like this:\n",
    "\n",
    "\n",
    "```\n",
    "debug_inverted_index(inverted_index, [\"elephant\", \"science\"], showTerms=True)\n",
    "\n",
    "**** elephant - 7 docs, 18 occurrences\n",
    "  at {8952-id_12 [259]} {38481-art31 [317, 438, 464]} {38482-art25 [3, 24, 47]} {8391-id_13 [5, 35, 80, 106, 350, 1014, 1225, 1466]} {8504-id_30 [38]} {24322-art09 [647]} {15193-art17 [396]}\n",
    "**** science - 112 docs, 251 occurrences\n",
    "  at {8951-id_9 [35]} {8951-id_21 [171]} {8952-id_15 [1561]} {8952-id_30 [40, 69, 120]} {8952-id_31 [5, 108, 134, 172, 406, 429, 485, 493, 559, 623, 739, 780, 956, 1063, 1072, 1126, 1183, 1391, 1684, 1693, 1941, 2015, 2172, 2222, 2302, 2343, 2392, 2404, 2551, 2564]} {8952-id_33 [196, 244, 524]} {8952-id_44 [112]} {8952-id_46 [18]} {8952-id_69 [9595, 15195]} {19180-art22 [1645]} {19180-art28 [90, 152]} {19180-art33 [131, 389]} {19180-art35 [277, 302, 565, 701, 852, 926]} {19180-art41 [152]} {19180-art53 [4613, 4625, 10221, 11314, 11326, 17166]} {19406-art48 [329, 554, 862, 1537]} {19406-art31 [5]} {19406-art44 [502, 726]} {19406-art68 [892]}\n",
    "```\n",
    "\n",
    "(For reasons of space, the output for `science` is truncated and the output for `and` as well as `the` is not shown at all.)\n",
    "\n",
    "Reminder, providing these debug functions is entirely optional. The autograder will not test whether you have any code that does that. However, you are likely to find something like this helpful as you debug your code -- e.g., it gives you a list of the documents that your query processing should return for simple one-word queries. Also, it's fair for the teaching staff to ask you for something comparable when you post a question that appears like it might be the result of incorrect indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "B5PUlV88JBRu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 34896 occurrences 956 docs\n",
      "the 96151 occurrences 966 docs\n",
      "elephant 18 occurrences 7 docs\n",
      "science 251 occurrences 112 docs\n"
     ]
    }
   ],
   "source": [
    "def debug_inverted_index(inverted_index, terms, showTerms=True):\n",
    "    \"\"\"\n",
    "        Print out debugging information for the index\n",
    "\n",
    "        Args:\n",
    "            inverted_index: the inverted index data structure.\n",
    "            terms: an array of strings, representing the terms to debug\n",
    "            showTerms: defaults to true, will print out inverted list instead of # of docs and occurrences for each term\n",
    "\n",
    "        Returns: n/a\n",
    "      \"\"\"\n",
    "    for term in terms:\n",
    "        if term in inverted_index.keys():\n",
    "            posting = inverted_index[term]\n",
    "            print(term, posting.total_term_count, \"occurrences\", posting.get_document_occurence(), \"docs\")\n",
    "            \n",
    "\n",
    "# Example usage\n",
    "debug_inverted_index(inverted_index, [\"and\", \"the\", \"elephant\",\"science\"], showTerms=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGAxufCzKO4h"
   },
   "source": [
    "<a name=\"query-processing\"></a>\n",
    "## 2. Read and process a list of queries from `query_list`\n",
    "\n",
    "The second component of `index_run()` requires you to run a sequence of queries that are provided in *queries.tsv*. You will want to have read the appropriate parts of Chapter 7 in the text. Recall that *sciam.json.gz* is the set of documents you index and *queries.tsv* is a list of queries to run. We are also providing *P3train.tsv* and *P3train.trecrun* which are a set of sample queries and their expected output, respectively.\n",
    "\n",
    "The list of queries (in *P3train.tsv* and in *queries.tsv*) comprises lines with at least three fields separated by tabs (tsv = tab separated values) in the format:\n",
    "\n",
    "> queryType queryName wordPhrase1 wordPhrase2 … wordPhraseN\n",
    "\n",
    "where:\n",
    "\n",
    "- *queryType* is one of the following, indicating what type of query you will be processing (case does not matter, so AND, and, And, aNd, and so on, are all the same):\n",
    " - **AND** means a Boolean query that is the AND of all of the *wordPhrases* listed.\n",
    " - **OR** means a Boolean query that is the OR of all of the *wordPhrases* listed.\n",
    " - **QL** means you will run a query likelihood algorithm with the *wordPhrases* as the query. QL is discussed starting in 7.3.1 of the textbook. You are asked to implement the version that uses Dirichlet smoothing, discussion of which starts on page 258 of both versions of the textbook. Use μ=300.\n",
    " - **BM25** means you will use the BM25 algorithm with the *wordPhrases* being the query. BM25 scoring is described starting on page 250 of the PDF or printed textbook. Please use these values for your scoring: k_1=1.8, k2=5, b=0.75.\n",
    " - **TF** means you will calculate the raw term frequency of *wordPhrase* for each story.\n",
    " - **DF** means you will calculate the document frequency of each *wordPhrase* listed (yes, even if it is a phrase). Your output (see below) will show the *wordPhrase* as if it were a document and the DF for the score. Rank will be ignored.\n",
    "- *queryName* is a provided name for this query (e.g., \"query1\" or \"12354\" or \"smelt\"). The first column of every output line will be this *queryName* (for this query).\n",
    "- *wordPhraseK* are the words/phrases that you need to find using your index.\n",
    " - If there are multiple words (separated by spaces) in a *WordPhrase*, it means a phrase and that those words must occur adjacent in an article and in that given order. Although phrases are often indicated by quotation marks elsewhere, they are _not_ used here.\n",
    " - Note that there will always be at least one *wordPhrase* and there is no theoretical limit on the value of N.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "565kqXUNDAmI"
   },
   "source": [
    "###**Handling phrases**\n",
    "\n",
    "If a *wordPhrase* contains one or more spaces, it is a phrase. (Recall that the *wordPhrases* are separated by tabs, so there can be spaces within them.) A phrase requires a match with two or more adjacent words in each document.\n",
    "\n",
    "For QL and BM25 (and DF), you need to know the number of times a phrase occurs in a document. (For Boolean you just have to know that number is at least one for a given document.) To do that, you'll have to actually do the equivalent of running that phrase as if it were a query. Remember that you are only interested in phrases, where the words occur next to each other in the order given; you do not need to handle things like “within 5 words” or “in the same sentence.”\n",
    "\n",
    "Although TF and DF and essentially index-probing \"query\" types, you must allow multiple *wordPhrase* occurrences as in all of the others and expect them to be phrases or single words. For TF, you will be scoring and ranking the documents by the TF count (with ties broken appropriately). For DF, your output will display the *wordPhrase* where the document would normally be, the DF value where the score would be, and the rank will be ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w42CvQFDGEs"
   },
   "source": [
    "<a name=\"output-formatting\"></a>\n",
    "###**Output format**\n",
    "\n",
    "The output of your program will be a file called *output.trecrun* containing a number of lines per query in *queries.tsv*. You are going to use the \"TREC run format\" as follows. For each query, you will display its ranked list using exactly six columns per line. White space (spaces or tabs) is used to separate columns. The width of the columns in the format is not important, but it is important to have exactly six columns per line with at least one space between the columns (note that no queryName or storyID contains a space). The format is,\n",
    "\n",
    "> *queryName* skip *storyID* rank score *yourUsername*\n",
    "\n",
    "where\n",
    "\n",
    "- the first column is the *queryName* from the file *queries.tsv*\n",
    "- the second column is currently unused and should always be `skip`.\n",
    "- the third column is the value of the *storyID* JSON field of this ranked document. (For the DF query type, it should be the *wordPhrase*.)\n",
    "- the fourth column is the rank of the document retrieved. You will list things sorted by rank within the query, with the highest score having rank 1 and there being no ties (even though there might be ties in the scores; break ties by sorting by *storyID*). The rank will be ignored for the DF type, though you must provide a value.)\n",
    "- the fifth column shows the score (integer or floating point) that generated the ranking. This score must be in descending (non-increasing) order. The score for a Boolean query will always be 1.0. The score for TF and DF query types should be the term frequency or document frequency, respectively.\n",
    "- the sixth column is called the \"run tag\" and is traditionally a unique identifier for you and for the method used. In this case, just use your SPIRE username (e.g., \"allan\" for the professor). Please do not turn in results with \"username\" (or \"allan\") as the identifier.\n",
    "\n",
    "\n",
    "Here is an example of what this might look like (the information is made up; see sample output for real results). It aligns the columns to make them easier to read; you do not have to do that. For example,\n",
    "```\n",
    "Q1    8951-id_20           1    1.000   username\n",
    "Q1    19406-art03          2    0.500   username\n",
    "Q1    38481-artnq13        3    0.333   username\n",
    "Q1    21081-article44-11   4    0.250   username\n",
    "…\n",
    "Q1    13443-elec1          237  0.003   username\n",
    "Qtwo  38481-artnq13        1    0.998   username\n",
    "Qdf   some phrase          1    10      username\n",
    "…\n",
    "```\n",
    "**Note that all queries are printed to _output.trecrun_** (the filename passed into eval below), **with all matching stories per query. For QL, BM25, and TF, a story matches a ranked query if it contains at least one of the query terms; it matches a Boolean query if it satisfies the Boolean expression.** If any QL, BM25, AND, OR query happens to retrieve zero documents, there will be no lines in the output file for that query. If any TF query has 0 term frequency for some document, that document should be omitted from your output. Any term frequency greater than zero for any document should otherwise be printed out to *output.trecrun*.\n",
    "\n",
    "If any *wordPhrase* in a DF query has 0 document frequency, there will be no line in the output file for that *wordPhrase*. Also note that for the DF measure, the *wordPhrase* instances are independent of each other: they are not processed as a group as they are for the other items. Just output one line per *wordPhrase* word or phrase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfSED4v6KR37"
   },
   "source": [
    "### 2.1 Running Boolean queries\n",
    "Your program will be expected to handle queries that find words, phrases (two or more adjacent words), or combinations of both. The queries will return the complete set of *storyID*s that match the Boolean query. Obviously there will never be more than the total number of stories in the collection.\n",
    "\n",
    "There are no scores other than 1.0/0.0 calculated, so sort the matching documents (all of which have a score of 1.0) by the *storyID* string (as a string, ignoring that it might look like a number in some ways -- i.e., “11-x” comes before “2-x”) and print out the list that way, increasing the rank from 1 through the number that match.\n",
    "\n",
    "Below are sample helper methods that you can use to help implement boolean query processing for `index_run()`. However, remember that you are not required to use this method, and will not be graded on the method itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfigiQaEOF9w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('improvement', 0): [0], ('improvement', 5): [16989], ('improvement', 22): [984, 1211, 1231, 2493, 2683, 3439, 3511, 3687], ('improvement', 95): [7991, 12266], ('improvement', 168): [799], ('improvement', 217): [2777, 5836], ('improvement', 276): [4988, 5155, 9051, 10577, 11108, 11625, 12311, 12639], ('improvement', 336): [2694, 3931, 4021, 5258, 6749, 15336, 15393, 15952, 16009], ('improvement', 449): [4159, 5191, 5740, 5873, 5913, 9140, 9561], ('improvement', 489): [1668, 1778, 2440], ('improvement', 741): [89], ('in', 0): [1, 93, 95, 120, 134, 222, 268, 275, 284, 450, 475, 489], ('in', 5): [118, 219, 252, 393, 431, 492, 527, 547, 578, 645, 717, 814, 902, 1039, 1076, 1102, 1159, 1163, 1194, 1209, 1262, 1295, 1394, 1726, 2022, 2038, 2041, 2081, 2129, 2152, 2167, 2220, 2296, 2373, 2391, 2496, 2519, 2573, 2622, 2674, 2704, 2757, 2776, 2841, 2888, 2970, 2980, 2998, 3014, 3055, 3141, 3319, 3336, 3374, 3632, 3640, 3673, 3755, 3788, 3810, 3833, 4203, 4266, 4379, 4432, 4477, 4541, 4619, 4705, 4721, 4746, 4778, 4795, 4872, 5113, 5398, 5429, 5435, 5456, 5604, 5697, 5754, 5810, 5816, 5858, 5872, 5932, 6120, 6232, 6290, 6340, 6354, 6387, 6415, 6438, 6453, 6489, 6504, 6744, 6876, 6912, 6925, 6939, 6946, 6952, 6973, 7004, 7008, 7016, 7021, 7068, 7118, 7123, 7168, 7173, 7228, 7257, 7377, 7462, 7492, 7608, 7622, 7635, 7834, 7846, 7856, 7877, 7918, 8047, 8092, 8127, 8140, 8172, 8224, 8264, 8284, 8303, 8332, 8346, 8355, 8374, 8391, 8398, 8431, 8445, 8550, 8645, 8747, 8760, 8772, 8790, 8943, 9025, 9055, 9074, 9130, 9151, 9159, 9182, 9204, 9214, 9329, 9351, 9424, 9489, 9509, 9561, 9573, 9636, 9923, 10055, 10245, 10288, 10330, 10377, 10492, 10652, 10671, 10726, 10892, 10925, 10978, 11001, 11126, 11151, 11403, 11564, 11602, 11675, 11798, 11819, 11899, 11973, 12101, 12216, 12227, 12266, 12327, 12395, 12808, 12812, 12838, 12991, 13027, 13071, 13158, 13199, 13375, 13389, 13393, 13483, 13499, 13520, 13560, 13682, 13707, 13749, 13799, 13939, 13972, 14038, 14046, 14058, 14128, 14341, 14430, 14490, 14652, 14728, 14764, 14794, 14805, 14811, 14818, 14825, 14855, 14962, 15004, 15024, 15416, 15446, 15484, 15513, 15630, 15666, 15733, 15751, 15767, 15800, 15810, 15833, 15886, 15925, 15953, 16005, 16030, 16065, 16305, 16369, 16471, 16491, 16621, 16634, 16647, 16686, 16715, 16722, 16725, 16810, 16817, 16836, 16851, 16899, 16906, 16925, 16954, 16964, 16973, 16980, 16993, 17029, 17036, 17154, 17163, 17186, 17221, 17283, 17351, 17497, 17508, 17601, 17642, 17706, 17770, 17895, 17924, 17949, 18153, 18180, 18195, 18244, 18272, 18567, 18620, 18699, 18780, 18786, 18811, 19412, 19442, 19533, 19619, 19777, 19858, 19884, 20046, 20117, 20193, 20419, 20469, 20485, 20541, 20715, 20773, 20779, 20879, 20971, 20999, 21234, 21261, 21296, 21332, 21358, 21365, 21465, 21556, 21712, 21792, 21815, 21824, 21938, 21954, 21969, 22037, 22067, 22196, 22250, 22331, 22338, 22370, 22705, 22773, 22812, 22871, 22918, 22928, 22996, 23040, 23060, 23221, 23245, 23381, 23434, 23505, 23586, 23606, 23636, 23674, 23690, 23759, 23784, 23797, 23821, 23833, 23848, 23926, 23936, 24017, 24072, 24163, 24220, 24264, 24270, 24307, 24326, 24344, 24361, 24391, 24436, 24471, 24493, 24514, 24519, 24563, 24611, 24670, 24685, 24716, 24803, 24828, 24850, 24871, 24897, 24910, 24929, 24982, 25162, 25205, 25278, 25340, 25346, 25425, 25448, 25511, 25576, 25647, 25657, 25682, 25691, 25796, 25837, 25840, 25885, 25934, 25948, 25997, 26004, 26063], ('in', 22): [32, 56, 104, 126, 137, 196, 228, 261, 345, 399, 427, 507, 511, 583, 668, 786, 853, 913, 985, 1038, 1078, 1112, 1150, 1157, 1212, 1233, 1281, 1351, 1450, 1499, 1527, 1660, 1676, 1870, 1876, 1887, 1964, 1979, 1995, 2008, 2045, 2130, 2138, 2160, 2171, 2220, 2226, 2232, 2280, 2345, 2433, 2453, 2486, 2495, 2523, 2535, 2684, 2689, 2786, 2796, 2817, 2832, 2843, 2867, 2875, 2923, 3031, 3079, 3121, 3150, 3198, 3213, 3304, 3421, 3440, 3463, 3476, 3512, 3521, 3554, 3558, 3574, 3583, 3622, 3688, 3695, 3699, 3781, 3823, 3869, 3882, 3964, 3989], ('in', 95): [105, 226, 948, 985, 2204, 2405, 2849, 3113, 3337, 4755, 5099, 5596, 5713, 5737, 6255, 6631, 6685, 6724, 6741, 6757, 6825, 6897, 6956, 6959, 7073, 7267, 7297, 7345, 7359, 7428, 7555, 7599, 7608, 7672, 7869, 7874, 7877, 7899, 7924, 7975, 8001, 8080, 8139, 8148, 8198, 8384, 8441, 8456, 8470, 8480, 8628, 8631, 8701, 8740, 8915, 9144, 9194, 9265, 9268, 9373, 9461, 9660, 9679, 9722, 9784, 9884, 9913, 9940, 9965, 10342, 10842, 10898, 10913, 11077, 11408, 11493, 11651, 11696, 11751, 11794, 11911, 11976, 12054, 12066, 12077, 12133, 12199, 12255, 12295, 12318, 12421, 12544, 12598, 12713, 12804, 12850, 12860, 12866, 12924, 12980, 12982, 12985, 13072, 13225, 13256, 13259, 13304, 13327, 13363, 13381, 13416, 13437, 13513, 13584, 14065, 14076, 14109, 14126, 14159, 14313, 14479, 14587, 15021, 15057, 15061, 15223, 15281, 15286, 15343, 15459, 15545, 15656], ('in', 168): [44, 94, 104, 170, 210, 256, 497, 565, 568, 665, 698, 807, 815, 819, 833, 898, 1022, 1226, 1277, 1297, 1318, 1350, 1365, 1377], ('in', 217): [13, 37, 56, 1837, 1857, 1873, 1903, 1979, 2024, 2102, 2120, 2126, 2177, 2180, 2231, 2296, 2394, 2413, 2499, 2526, 2594, 2626, 2671, 2707, 2725, 2728, 2735, 2744, 2805, 2929, 3107, 3165, 3189, 3197, 3272, 3279, 3282, 3393, 3421, 3435, 3457, 3526, 3547, 3588, 3604, 3639, 3660, 3771, 3941, 3952, 3967, 3987, 4016, 4030, 4065, 4072, 4083, 4128, 4176, 4282, 4285, 4312, 4343, 4351, 4416, 4454, 4626, 4701, 4760, 4799, 4858, 5056, 5091, 5114, 5163, 5173, 5198, 5274, 5277, 5285, 5308, 5323, 5706, 5887, 5946, 5977, 6112, 6151, 6280, 6371, 6621, 6628, 6636, 6643, 6698, 6862, 7063, 7130, 7161, 7187, 7244, 7320], ('in', 276): [14, 38, 57, 993, 1936, 1967, 2016, 2057, 2105, 2222, 2283, 2316, 2390, 2442, 2569, 2575, 2603, 2692, 2989, 3050, 3251, 3366, 3425, 3474, 3515, 3569, 3622, 3744, 3805, 3838, 3917, 4019, 4151, 4157, 4185, 4274, 4571, 4632, 4833, 5061, 5082, 5235, 5256, 5274, 5295, 5412, 5500, 5523, 5645, 5660, 5674, 5708, 5731, 5757, 5797, 5816, 5856, 5896, 5901, 5937, 5940, 6014, 6098, 6212, 6229, 6275, 6384, 6424, 6431, 6434, 6545, 6573, 6630, 6637, 6640, 6751, 6779, 6914, 6970, 6995, 6998, 7025, 7050, 7062, 7176, 7298, 7409, 7465, 7534, 7568, 7571, 7598, 7623, 7635, 7755, 7882, 8042, 8158, 8288, 8353, 8418, 8461, 8510, 8548, 8570, 8746, 8848, 8879, 8930, 8961, 8968, 8976, 8983, 9039, 9061, 9092, 9211, 9223, 9238, 9246, 9285, 9543, 9664, 9800, 9869, 9934, 9984, 10033, 10071, 10093, 10269, 10371, 10402, 10453, 10484, 10491, 10499, 10506, 10565, 10587, 10618, 10737, 10749, 10764, 10772, 10811, 11259, 11388, 11454, 11463, 11482, 11776, 11891, 12046, 12112, 12121, 12140, 12179, 12184, 12312, 12315, 12640, 12643, 12899, 13004, 13011, 13017, 13172, 13184, 13366, 13557, 13715, 13727, 13909, 14108], ('in', 336): [23, 62, 108, 218, 304, 322, 334, 419, 457, 609, 633, 652, 2516, 2572, 2774, 2888, 2894, 2900, 3055, 3141, 3244, 3416, 3433, 3455, 3463, 3473, 3496, 3513, 3525, 3628, 3679, 3711, 3714, 3755, 3760, 3803, 3817, 3821, 3863, 3880, 3886, 3926, 3959, 3965, 3987, 3994, 4101, 4215, 4221, 4227, 4382, 4468, 4571, 4743, 4760, 4782, 4790, 4800, 4823, 4840, 4852, 4955, 5006, 5038, 5041, 5082, 5087, 5130, 5144, 5148, 5190, 5207, 5213, 5253, 5286, 5292, 5314, 5321, 5346, 5415, 5433, 5527, 5571, 5673, 5705, 5747, 5790, 5840, 5896, 5914, 5940, 5971, 5999, 6079, 6170, 6429, 6438, 6533, 6621, 6639, 6660, 6737, 6759, 6790, 6900, 6912, 6927, 6935, 6974, 7246, 7421, 7448, 7457, 7565, 7612, 7681, 7698, 7708, 7785, 7846, 7887, 7943, 8202, 8441, 8491, 8679, 8706, 8715, 8823, 8875, 8944, 8961, 8971, 9055, 9116, 9157, 9218, 9482, 9737, 9802, 9834, 10091, 10335, 10441, 10565, 10628, 10740, 10885, 11062, 11102, 11147, 11269, 11309, 11354, 11490, 11520, 12016, 12034, 12041, 12304, 12321, 12354, 12361, 12364, 12474, 12502, 12543, 12560, 12654, 12692, 12736, 13081, 13099, 13106, 13410, 13427, 13467, 13474, 13477, 13587, 13615, 13661, 13678, 13776, 13820, 13854, 13953, 14039, 14119, 14210, 14274, 14508, 14809, 14827, 15079, 15153, 15218, 15237, 15245, 15272, 15430, 15448, 15459, 15506, 15532, 15537, 15679, 15753, 15818, 15837, 15845, 15872, 16046, 16064, 16075, 16122, 16148, 16153, 16260, 16297, 16337, 16374, 16418, 16458], ('in', 449): [51, 88, 193, 222, 242, 731, 2594, 2641, 2697, 2756, 2761, 3054, 3066, 3123, 3334, 3368, 3375, 3409, 3498, 3523, 3622, 3863, 3941, 3991, 4022, 4025, 4084, 4093, 4162, 4336, 4339, 4345, 4386, 4492, 4575, 4606, 4683, 4879, 4906, 5101, 5117, 5127, 5199, 5364, 5382, 5687, 5725, 5746, 5820, 5858, 5879, 5964, 6106, 6117, 6157, 6170, 6193, 6238, 6255, 6271, 6279, 6325, 6342, 6372, 6489, 6548, 6576, 6579, 6614, 6664, 6690, 6746, 6785, 6833, 6851, 6862, 6879, 7075, 7155, 7164, 7387, 7434, 7449, 7607, 7663, 7701, 7704, 7710, 7713, 7750, 7819, 7897, 7999, 8033, 8126, 8131, 8271, 8287, 8364, 8462, 8639, 8684, 8752, 8769, 8830, 8895, 8956, 9178, 9241, 9268, 9283, 9340, 9370, 9385, 9578, 9741, 9798, 9816, 9842, 9873, 9901, 9979, 10070], ('in', 489): [37, 51, 55, 59, 112, 121, 163, 216, 244, 257, 396, 409, 438, 440, 454, 467, 474, 496, 511, 580, 608, 627, 665, 695, 711, 839, 890, 903, 918, 972, 1057, 1075, 1085, 1108, 1157, 1199, 1217, 1223, 1297, 1354, 1453, 1488, 1510, 1580, 1588, 1685, 1779, 1809, 1886, 1940, 1959, 2023, 2054, 2071, 2110, 2123, 2131, 2156, 2171, 2176, 2241, 2259, 2264, 2276, 2321, 2342, 2381, 2435, 2537, 2558, 2602, 2641, 2739, 2792, 2803, 2808, 2812, 2916, 2937, 2983, 3030, 3033, 3062, 3072, 3092, 3119, 3145, 3150, 3153, 3176, 3197, 3228, 3341, 3346, 3351, 3379, 3403, 3518, 3547, 3555, 3683, 3750, 3796, 3824, 3845, 3865, 3907, 4054, 4070, 4087, 4123, 4150, 4193], ('in', 741): [94, 105, 111, 155, 165, 240, 260, 281, 339, 351, 395, 400, 403, 407, 418, 459, 497, 498, 532, 539, 556, 600, 605], ('hull', 0): [2, 13, 33, 54, 131, 139, 289, 359, 415, 426, 463], ('hull', 5): [21873], ('hull', 22): [1514], ('hull', 95): [10576], ('hull', 168): [420], ('hull', 217): [4875], ('hull', 276): [12227, 12550, 13098, 13641], ('hull', 336): [1051], ('hull', 449): [1858, 2128], ('hull', 489): [2159], ('hull', 741): [103]}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def filter_postings(wordPhrase: str, single_word: bool) -> dict[(str, int): list[int]]:\n",
    "    \"\"\"\n",
    "    wordPhrase is excpeted to include a single space with > 1 word\n",
    "    for example: united states, unicorn pony\n",
    "    \"\"\"\n",
    "    word_info = {} # {(word: str, doc_id: int): positions: list[int]}\n",
    "    split_phrase = [word for word in wordPhrase.split() if word != \"\"] # split off of spaces and don't include empty strings\n",
    "    doc_id_lst = []\n",
    "\n",
    "    if all(True if word in inverted_index.keys() else False for word in split_phrase):\n",
    "        for word in split_phrase:\n",
    "            tmp = [] # holds doc_ids of which the current word of iteration occurs in\n",
    "            for doc_id in inverted_index[word].doc_to_term.keys():\n",
    "                tmp.append(doc_id)\n",
    "            doc_id_lst.append(tmp) # list of lists, each individually represents what docs contain which term, where the indice is the key\n",
    "\n",
    "        combined_lst = []\n",
    "        if not single_word:\n",
    "            for i in range(1, len(split_phrase)):\n",
    "                cur_lst = doc_id_lst[i]     # docs that element 1 occurs in\n",
    "                prev_lst = doc_id_lst[i-1]  # docs that element 2 occur in \n",
    "                combined_lst.append(set(cur_lst).intersection(set(prev_lst))) # combined_lst gives use documents that these elements occur in\n",
    "                \n",
    "            combined_lst = set.intersection(*combined_lst)\n",
    "            if len(combined_lst) == 0:\n",
    "                return None # if intersection results elements sharing no common documents, then the phrase cannot be returned properly.\n",
    "        else:\n",
    "            combined_lst = tmp\n",
    "        \n",
    "        for word in split_phrase:\n",
    "            for doc_id, items in inverted_index[word].doc_to_term.items():\n",
    "                if doc_id in combined_lst:\n",
    "                    word_info[(word, doc_id)] = items[1]\n",
    "        return word_info\n",
    "\n",
    "def intersect(inverted_index, wordPhrases):\n",
    "    \"\"\"\n",
    "      Evaluate an AND boolean query over all wordPhrases in the query\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          wordPhrases: a list of words or phrases to search for.\n",
    "\n",
    "      Returns: an array of tuples, where the first value is the doc_id, and the second is the score\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    wordPhrase_lst = wordPhrases.split(\"\\t\")\n",
    "    filtered_postings = filter_postings(wordPhrase_lst[0], single_word=False) # dict[(word, doc_id): positions[int]]\n",
    "    \n",
    "    # below we can use a similar method, but need to use the above implementation.\n",
    "    # if len(check_phrase) != len(wordPhrases): return results # if these lengths do not match, the intersect must fail, one can assume all doc_id pair values will be 0.0\n",
    "    # doc_lst = []\n",
    "    # for phrase in check_phrase:\n",
    "    #     doc_lst.append([(doc_id, inverted_index[phrase].doc_to_id[doc_id]) for doc_id in inverted_index[phrase].doc_to_term.keys()])  # for each term, generates a list of doc_ids that contain this term\n",
    "    # intersect_doc_ids = set(doc_lst[0]).intersection(*doc_lst[1:])   # filters out the doc_ids that don't contain all query values\n",
    "    # for pair in intersect_doc_ids:     # any documents that doesn't have a generateted value is assumed to be zero\n",
    "    #     results.append((pair[1], 1.0))\n",
    "    \n",
    "    return results\n",
    "\n",
    "test_phrase = \"improvement in hull\\tnitro glycerin professor\"\n",
    "print(intersect(inverted_index, test_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GAT1Op5YOG8F"
   },
   "outputs": [],
   "source": [
    "def union(inverted_index, wordPhrases):\n",
    "    \"\"\"\n",
    "      Evaluate an OR boolean query over all wordPhrases in the query\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          wordPhrases: a list of words or phrases to search for.\n",
    "\n",
    "      Returns: a set of tuples, where the first value is the doc_id, and the second is the score\n",
    "    \"\"\"\n",
    "\n",
    "    results = set()\n",
    "\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erwaOuSsKjUU"
   },
   "source": [
    "### 2.2 Running QL and BM25 queries\n",
    "\n",
    "The QL and BM25 approaches calculate a score that is intended to reflect the probability of relevance in some way. That score produces the ranking.\n",
    "\n",
    "For QL, the score is the product of a number of small numbers (probabilities), which runs the risk of creating arithmetic underflow. Represent probabilities as the log (**natural log**) of the probabilities and sum them. That means that the score for these ranking runs will be negative, since the log of a number less than one (e.g., a probability) will be negative. Sorting in reverse order by the negative scores will put higher probabilities at the start of the list as you want.\n",
    "\n",
    "For these ranking queries, only print documents at ranks 1 through 100, inclusive, unless the query matches fewer than 100 stories, in which case list everything retrieved -- **that is, any story that includes at least one of the query terms**. If there are documents with the same score, break the tie by sorting those by the storyID textual field's value (as a string, ignoring that it looks like a number in some ways -- i.e., “11-x” comes before “2-x”).\n",
    "\n",
    "Below are sample helper methods that you can use to help implement QL and BM25 query processing for `index_run()`. They copy the constant values that were listed above for ease of reference. However, you are not required to use this method, and will not be graded on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gd-uU6eGOKkO"
   },
   "outputs": [],
   "source": [
    "def query_likelihood(inverted_index, wordPhrases):\n",
    "    \"\"\"\n",
    "      Evaluate a QL query over all wordPhrases\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          wordPhrases: a list of words or phrases to search for.\n",
    "\n",
    "      Returns: a set of tuples, where the first value is the doc_id, and the second is the score\n",
    "    \"\"\"\n",
    "    # need to keep track of number of words per document, maybe edit data structure\n",
    "    ranking = []\n",
    "    mu = 300  # Dirichlet smoothing parameter\n",
    "\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WHf2GC3EOMwo"
   },
   "outputs": [],
   "source": [
    "def bm25(inverted_index, words):\n",
    "    \"\"\"\n",
    "      Perform a bm25 query over all words in the query\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          words: a list of words or phrases to search for.\n",
    "\n",
    "      Returns: an array of tuples, where the first value is the doc_id, and the second is the score\n",
    "    \"\"\"\n",
    "    # need tf, document length, average document length, N (# of docs in collection)\n",
    "    results = []\n",
    "    k1 = 1.8\n",
    "    k2 = 5.0\n",
    "    b = 0.75\n",
    "\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khK8rRdi8IMT"
   },
   "source": [
    "###2.3 Running TF and DF queries\n",
    "\n",
    "TF and DF calculate a score that is intended to reflect the raw term frequency or document frequency, respectively. That score produces the ranking.\n",
    "\n",
    "For TF, the score for a document is the raw term frequency, or the number of times a *wordPhrase* occurs in the document. For any TF query, we will calculate the raw term frequency across all documents, and rank according to the raw term frequency. Ties should be broken according to the *storyID* (as in AND and OR). If the TF is 0 for any document, do not list that document in the output.\n",
    "\n",
    "For DF, the output is a bit different. The \"storyID\" field will be replaced by the *wordPhrase*, the score will be the document frequency for that *wordPhrase*, and the rank will be ignored. If the DF is 0 for the given *wordPhrase*, we will not print anything in *output.trecrun* for that *wordPhrase*. **NOTE: If the _wordPhrase_ is a phrase -- i.e., it contains spaces -- replace all of the spaces with underscores.** That is because the trecrun file format does not allow spaces in docids and we want to be consistent with that format. So `DF qname united<tab>states<tab>united states` woudl result in one line for `united`, another for `states`, and one for the phrase that would look liked `united_states`. Note that if the query were actually `DF qname united_states` the output would look the same (`united_states`) but the DF count would almost certainly be different.\n",
    "\n",
    "Below are sample helper methods that you can use to help implement TF and DF query processing for `index_run()`. However, you are not required to use this method, and will not be graded on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cnppR3He8me1"
   },
   "outputs": [],
   "source": [
    "def tf(inverted_index, doc_id, words):\n",
    "    \"\"\"\n",
    "      Calculate the number of times a term or phrase appears in a document\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          doc_id: the document we want to calculate the term frequency for.\n",
    "          term: the term or phrase we want to calculate the frequency for.\n",
    "\n",
    "      Returns: an integer representing the number of times the term appears in doc_id.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cEHOUbK08n1S"
   },
   "outputs": [],
   "source": [
    "def df(inverted_index, wordPhrase):\n",
    "    \"\"\"\n",
    "      Calculate the number of documents a term or phrase appears in\n",
    "      NOTE that this sample only handles one wordPhrase at at time and so only returns\n",
    "      a single number. This is primarily to highlight that it operates differently\n",
    "      than the others that are ranking documents.\n",
    "\n",
    "      Args:\n",
    "          inverted_index: the inverted index data structure.\n",
    "          term: the term or phrase we want to calculate the frequency for.\n",
    "\n",
    "      Returns: an integer representing the number of documents a term or phrase appears in.\n",
    "    \"\"\"\n",
    "\n",
    "    doc_freq = 0\n",
    "\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "\n",
    "    return doc_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6prrF_LGGLP"
   },
   "source": [
    "## 3. Implement `run_queries()`\n",
    "\n",
    "This is the only method that you will be graded on, which should create your inverted index based on the file named by `documents` (such as *sciam.json.gz*), parse and evaluate all queries in `queries_list` (as in *queries.tsv*), and print out the results to a file named by `trecrun_file` (for example, *output.trecrun*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "ldaX0-z3HvMC"
   },
   "outputs": [],
   "source": [
    "def run_queries(documents, query_list, trecrun_file):\n",
    "    \"\"\"\n",
    "      Create an inverted index, parse a query file, run them on the inverted index,\n",
    "      and print out the results in trecrun format.\n",
    "\n",
    "      Args:\n",
    "          documents: the name of a file containing the documents to index\n",
    "          query_list: the name of the file containing queries to run\n",
    "          trecrun_file: the name of the file that will be created to contain the\n",
    "                        output of running the queries\n",
    "\n",
    "      Returns: void\n",
    "    \"\"\"\n",
    "\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "    return\n",
    "\n",
    "# Example usage (generates P3train.myrun to compare to P3train.trecrun)\n",
    "run_queries(\"sciam.json.gz\", \"P3train.tsv\", \"P3train.myrun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg17qtuS8_l7"
   },
   "source": [
    "### 3.1 Sample queries and answers\n",
    "\n",
    "We are providing *P3train.tsv* that you can use to try out your system. We are also providing you with the expected output for those queries as *P3train.trecrun*. Your code will be tested on those same queries plus a number of other queries, so please try other possibilities: do not assume that because your code handles the training queries your code is perfect. You may use the below method, *print_file()*, to print out *P3train.trecrun* to compare your results manually. You might want to create a variation that also reads in your output and does a comparison to find where things are different. However, this part will not be graded, and is optional to implement. As always, just make sure that it does not trigger an error in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "id": "XrgAN9L1PM4_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def print_file(file):\n",
    "    \"\"\"\n",
    "      Print out the contents of a file\n",
    "\n",
    "      Args:\n",
    "          file_path: the path to the file\n",
    "\n",
    "      Returns: n/a\n",
    "    \"\"\"\n",
    "\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "    print()\n",
    "\n",
    "# Example usage\n",
    "print_file(trecrun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EujWLMszLBmd"
   },
   "outputs": [],
   "source": [
    "def print_queryfile(file_path):\n",
    "    \"\"\"\n",
    "      Print out the contents of a file\n",
    "\n",
    "      Args:\n",
    "          file_path: the path to the file\n",
    "\n",
    "      Returns: n/a\n",
    "    \"\"\"\n",
    "    #########\n",
    "    ##\n",
    "    ## Implement the function here\n",
    "    ##\n",
    "    #########\n",
    "\n",
    "# Example usage\n",
    "print_queryfile(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2i8QsRrUdUop"
   },
   "source": [
    "## 4. Analysis Questions\n",
    "\n",
    "Answer the questions below. You will probably find it easiest to answer some of them if you engineer one of your functions above to display these numbers after indexing and then you can copy them below.\n",
    "\n",
    "Unlike the way P1 and P2 were implemented, these analysis questions are designed to be handled by the autograder, though they will be hidden \"tests\" so you will not have confirmation that you got them right until after grading is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX7JayJmdYdr"
   },
   "source": [
    "**4.1**. What is the average length of a story in the *sciam* collection? What is the shortest story (and how short it is)? What is the longest story (and how long is it)? Note that for this project, \"short\" and \"long\" are measured by the number of tokens, not the number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qTwBJjsbf_Ri"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2584224492.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [16], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    averageStoryLength =\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Provide the correct values for each of these \"variables\" as described above. Do not provide any other code.\n",
    "\n",
    "def analysis_q1():\n",
    "\n",
    "  averageStoryLength =\n",
    "  averageStoryStoryId = \"\"\n",
    "  shortestStoryLength =\n",
    "  longestStoryStoryId = \"\"\n",
    "  longestStoryLength =\n",
    "\n",
    "  return [averageStoryLength, averageStoryStoryId, shortestStoryLength, longestStoryStoryId, longestStoryLength]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5ShTxDede52"
   },
   "source": [
    "**4.2**. What word occurs in the most stories and how many stories does it occur in? What word has the largest number of occurrences and how many does it have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4ejE0afgtJZ"
   },
   "outputs": [],
   "source": [
    "# Provide the correct values for each of these \"variables\" as described above. Do not provide any other code.\n",
    "\n",
    "def analysis_q2():\n",
    "\n",
    "  termOccurringInMostStories = \"\"\n",
    "  numberOfStoriesItOccursIn =\n",
    "  termOccurringMostFrequently = \"\"\n",
    "  numberOfTimesItOccurs =\n",
    "\n",
    "  return [termOccurringInMostStories, numberOfStoriesItOccursIn, termOccurringMostFrequently, numberOfTimesItOccurs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzeT9jwfdhHI"
   },
   "source": [
    "**4.3.** How many unique words are there in this collection? How many of them occur only once? What percent is that? Is that what you would expect? We are not asking you to return this, but to increase the chance that you have the right answer, you might think about why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWuHE51Eg6lF"
   },
   "outputs": [],
   "source": [
    "# Provide the correct values for each of these \"variables\" as described above. Do not provide any other code.\n",
    "\n",
    "def analysis_q3():\n",
    "\n",
    "  numberUniqueWords =\n",
    "  numberWordsOccurringOnce =\n",
    "  percentOfWordsThatOccurOnce = # be sure this is a percent (without the %) and not a fraction\n",
    "\n",
    "  return [numberUniqueWords, numberWordsOccurringOnce, percentOfWordsThatOccurOnce]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "javDin3edkVO"
   },
   "source": [
    "**4.4**. Your training queries have two queries that are roughly about the scientific american supplement. Suppose that you wanted to judge stories for relevance using a pooling strategy that takes the top 100 documents from each of those two queries. How many unique documents will you be judging? What if you only considered the top 20? Suppose you had a budget that allowed you to judge at most 30 documents. How deeply could you go into the two queries for judging to get 30 judged, no more, no less?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34ZyZoQcfrpf"
   },
   "source": [
    "\n",
    "\n",
    ">Enter your response here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZ-UINLgQEhg"
   },
   "outputs": [],
   "source": [
    "# Provide the correct values for each of these \"variables\" as described above. Do not provide any other code.\n",
    "\n",
    "def analysis_q4():\n",
    "\n",
    "  numberUniqueJudged100 =\n",
    "  numberUniqueJudged20 =\n",
    "  poolDepthFor30 =\n",
    "\n",
    "  return [numberUniqueJudged100, numberUniqueJudged20, poolDepthFor30]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
